{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit 4 Capstone - Classifying classic authors by word usage\n",
    "\n",
    "For this project you'll dig into a large amount of text and apply most of what you've covered in this unit and in the course so far.\n",
    "\n",
    "First, pick a set of texts. This can be either a series of novels, chapters, or articles. Anything you'd like. It just has to have multiple entries of varying characteristics. At least 100 should be good. There should also be at least 10 different authors, but try to keep the texts related (either all on the same topic of from the same branch of literature - something to make classification a bit more difficult than obviously different subjects).\n",
    "\n",
    "This capstone can be an extension of your NLP challenge if you wish to use the same corpus. If you found problems with that data set that limited your analysis, however, it may be worth using what you learned to choose a new corpus. Reserve 25% of your corpus as a test set.\n",
    "\n",
    "The first technique is to create a series of clusters. Try several techniques and pick the one you think best represents your data. Make sure there is a narrative and reasoning around why you have chosen the given clusters. Are authors consistently grouped into the same cluster?\n",
    "\n",
    "Next, perform some unsupervised feature generation and selection using the techniques covered in this unit and elsewhere in the course. Using those features then build models to attempt to classify your texts by author. Try different permutations of unsupervised and supervised techniques to see which combinations have the best performance.\n",
    "\n",
    "Lastly return to your holdout group. Does your clustering on those members perform as you'd expect? Have your clusters remained stable or changed dramatically? What about your model? Is it's performance consistent?\n",
    "\n",
    "If there is a divergence in the relative stability of your model and your clusters, delve into why.\n",
    "\n",
    "Your end result should be a write up of how clustering and modeling compare for classifying your texts. What are the advantages of each? Why would you want to use one over the other? Approximately 3-5 pages is a good length for your write up, and remember to include visuals to help tell your story!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from nltk.corpus import gutenberg, stopwords\n",
    "from collections import Counter\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n"
     ]
    }
   ],
   "source": [
    "# get list of texts to load\n",
    "print(gutenberg.fileids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function for standard text cleaning.\n",
    "def text_cleaner(text):\n",
    "    # Visual inspection identifies a form of punctuation spaCy does not\n",
    "    # recognize: the double dash '--'.  Better get rid of it now!\n",
    "    text = re.sub(r'--',' ',text)\n",
    "    text = re.sub(\"[\\[].*?[\\]]\", \"\", text)\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "# load first 5000 characters from ten books by different authors as tuple\n",
    "austen = gutenberg.raw('austen-emma.txt')[0:100000]\n",
    "edgeworth = gutenberg.raw('edgeworth-parents.txt')[0:100000]\n",
    "blake = gutenberg.raw('blake-poems.txt')[0:100000]\n",
    "bryant = gutenberg.raw('bryant-stories.txt')[0:100000]\n",
    "burgess = gutenberg.raw('burgess-busterbrown.txt')[0:100000]\n",
    "carroll = gutenberg.raw('carroll-alice.txt')[0:100000]\n",
    "chesterton = gutenberg.raw('chesterton-ball.txt')[0:100000]\n",
    "melville = gutenberg.raw('melville-moby_dick.txt')[0:100000]\n",
    "milton = gutenberg.raw('milton-paradise.txt')[0:100000]\n",
    "shakespeare = gutenberg.raw('shakespeare-caesar.txt')[0:100000]\n",
    "# clean text\n",
    "austen = text_cleaner(austen)\n",
    "edgeworth = text_cleaner(edgeworth)\n",
    "blake = text_cleaner(blake)\n",
    "bryant = text_cleaner(bryant)\n",
    "burgess = text_cleaner(burgess)\n",
    "carroll = text_cleaner(carroll)\n",
    "chesterton = text_cleaner(chesterton)\n",
    "melville = text_cleaner(melville)\n",
    "milton = text_cleaner(milton)\n",
    "shakespeare = text_cleaner(shakespeare)\n",
    "# remove chapter and volume headings\n",
    "austen = re.sub(r'CHAPTER \\d+', '', austen)\n",
    "austen = re.sub(r'VOLUME \\d+', '', austen)\n",
    "edgeworth = re.sub(r'Chapter \\d+', '', edgeworth)\n",
    "blake = re.sub(r'Chapter \\d+', '', blake)\n",
    "bryant = re.sub(r'Chapter \\d+', '', bryant)\n",
    "burgess = re.sub(r'Chapter \\d+', '', burgess)\n",
    "carroll = re.sub(r'Chapter \\d+', '', carroll)\n",
    "chesterton = re.sub(r'Chapter \\d+', '', chesterton)\n",
    "melville = re.sub(r'Chapter \\d+', '', melville)\n",
    "milton = re.sub(r'Chapter \\d+', '', milton)\n",
    "shakespeare = re.sub(r'Chapter \\d+', '', shakespeare)\n",
    "# load spacy\n",
    "nlp = spacy.load('en')\n",
    "# load ten sentences from each doc author\n",
    "austen_doc = nlp(austen)\n",
    "edgeworth_doc = nlp(edgeworth)\n",
    "blake_doc = nlp(blake)\n",
    "bryant_doc = nlp(bryant)\n",
    "burgess_doc = nlp(burgess)\n",
    "carroll_doc = nlp(carroll)\n",
    "chesterton_doc = nlp(chesterton)\n",
    "melville_doc = nlp(melville)\n",
    "milton_doc = nlp(milton)\n",
    "shakespeare_doc = nlp(shakespeare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to create a list of the 2000 most common words.\n",
    "def bag_of_words(text):\n",
    "    \n",
    "    # Filter out punctuation and stop words.\n",
    "    allwords = [token.lemma_\n",
    "                for token in text\n",
    "                if not token.is_punct\n",
    "                and not token.is_stop]\n",
    "    \n",
    "    # Return the most common words.\n",
    "    return [item[0] for item in Counter(allwords).most_common(2000)]\n",
    "    \n",
    "\n",
    "# Creates a data frame with features for each word in our common word set.\n",
    "# Each value is the count of the times the word appears in each sentence.\n",
    "def bow_features(sentences, common_words):\n",
    "    \n",
    "    # Scaffold the data frame and initialize counts to zero.\n",
    "    df = pd.DataFrame(columns=common_words)\n",
    "    df['text_sentence'] = sentences[0]\n",
    "    df['text_source'] = sentences[1]\n",
    "    df.loc[:, common_words] = 0\n",
    "    \n",
    "    # Process each row, counting the occurrence of words in each sentence.\n",
    "    for i, sentence in enumerate(df['text_sentence'][::100]):\n",
    "        \n",
    "        # Convert the sentence to lemmas, then filter out punctuation,\n",
    "        # stop words, and uncommon words.\n",
    "        words = [token.lemma_\n",
    "                 for token in sentence\n",
    "                 if (\n",
    "                     not token.is_punct\n",
    "                     and not token.is_stop\n",
    "                     and token.lemma_ in common_words\n",
    "                 )]\n",
    "        \n",
    "        # Populate the row with word counts.\n",
    "        for word in words:\n",
    "            df.loc[i, word] += 1\n",
    "        \n",
    "        # This counter is just to make sure the kernel didn't hang.\n",
    "        if i % 50 == 0:\n",
    "            print(\"Processing row {}\".format(i))\n",
    "            \n",
    "    return df\n",
    "\n",
    "# Set up the bags.\n",
    "austenwords = bag_of_words(austen_doc)\n",
    "edgeworthwords = bag_of_words(edgeworth_doc)\n",
    "blakewords = bag_of_words(blake_doc)\n",
    "bryantwords = bag_of_words(bryant_doc)\n",
    "burgesswords = bag_of_words(burgess_doc)\n",
    "carrollwords = bag_of_words(carroll_doc)\n",
    "chestertonwords = bag_of_words(chesterton_doc)\n",
    "melvillewords = bag_of_words(melville_doc)\n",
    "miltonwords = bag_of_words(milton_doc)\n",
    "shakespearewords = bag_of_words(shakespeare_doc)\n",
    "\n",
    "# Combine bags to create a set of unique words.\n",
    "common_words = set(austenwords + edgeworthwords + blakewords + bryantwords + burgesswords + carrollwords + \n",
    "                   chestertonwords + melvillewords + miltonwords + shakespearewords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(VOLUME, I, CHAPTER)</td>\n",
       "      <td>Austen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(I)</td>\n",
       "      <td>Austen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(Emma, Woodhouse, ,, handsome, ,, clever, ,, a...</td>\n",
       "      <td>Austen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(She, was, the, youngest, of, the, two, daught...</td>\n",
       "      <td>Austen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(Her, mother, had, died, too, long, ago, for, ...</td>\n",
       "      <td>Austen</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0       1\n",
       "0                               (VOLUME, I, CHAPTER)  Austen\n",
       "1                                                (I)  Austen\n",
       "2  (Emma, Woodhouse, ,, handsome, ,, clever, ,, a...  Austen\n",
       "3  (She, was, the, youngest, of, the, two, daught...  Austen\n",
       "4  (Her, mother, had, died, too, long, ago, for, ...  Austen"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Group into sentences.\n",
    "austen_sents = [[sent, 'Austen'] for sent in austen_doc.sents]\n",
    "edgeworth_sents = [[sent, 'Edgeworth'] for sent in edgeworth_doc.sents]\n",
    "blake_sents = [[sent, 'Blake'] for sent in blake_doc.sents]\n",
    "bryant_sents = [[sent, 'Bryant'] for sent in bryant_doc.sents]\n",
    "burgess_sents = [[sent, 'Burgess'] for sent in burgess_doc.sents]\n",
    "carroll_sents = [[sent, 'Carroll'] for sent in carroll_doc.sents]\n",
    "chesterton_sents = [[sent, 'Chesterton'] for sent in chesterton_doc.sents]\n",
    "melville_sents = [[sent, 'Melville'] for sent in melville_doc.sents]\n",
    "milton_sents = [[sent, 'Milton'] for sent in milton_doc.sents]\n",
    "shakespeare_sents = [[sent, 'Shakespeare'] for sent in shakespeare_doc.sents]\n",
    "\n",
    "# Combine the sentences from the two novels into one data frame.\n",
    "sentences = pd.DataFrame(austen_sents + edgeworth_sents + blake_sents + bryant_sents + burgess_sents + carroll_sents\n",
    "                         + chesterton_sents + melville_sents + milton_sents + shakespeare_sents)\n",
    "sentences.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 0\n",
      "Processing row 50\n",
      "Processing row 100\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wood</th>\n",
       "      <th>freeze</th>\n",
       "      <th>concord</th>\n",
       "      <th>intelligent</th>\n",
       "      <th>johnny</th>\n",
       "      <th>xiii</th>\n",
       "      <th>faile</th>\n",
       "      <th>tie</th>\n",
       "      <th>mazy</th>\n",
       "      <th>scalding</th>\n",
       "      <th>...</th>\n",
       "      <th>flannel</th>\n",
       "      <th>so</th>\n",
       "      <th>summon</th>\n",
       "      <th>peers</th>\n",
       "      <th>hasten</th>\n",
       "      <th>traitors</th>\n",
       "      <th>drizzle</th>\n",
       "      <th>africa</th>\n",
       "      <th>text_sentence</th>\n",
       "      <th>text_source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(VOLUME, I, CHAPTER)</td>\n",
       "      <td>Austen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(I)</td>\n",
       "      <td>Austen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(Emma, Woodhouse, ,, handsome, ,, clever, ,, a...</td>\n",
       "      <td>Austen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(She, was, the, youngest, of, the, two, daught...</td>\n",
       "      <td>Austen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(Her, mother, had, died, too, long, ago, for, ...</td>\n",
       "      <td>Austen</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 8215 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  wood freeze concord intelligent johnny xiii faile tie mazy scalding  ...  \\\n",
       "0    0      0       0           0      0    0     0   0    0        0  ...   \n",
       "1    0      0       0           0      0    0     0   0    0        0  ...   \n",
       "2    0      0       0           0      0    0     0   0    0        0  ...   \n",
       "3    0      0       0           0      0    0     0   0    0        0  ...   \n",
       "4    0      0       0           0      0    0     0   0    0        0  ...   \n",
       "\n",
       "  flannel so summon peers hasten traitors drizzle africa  \\\n",
       "0       0  0      0     0      0        0       0      0   \n",
       "1       0  0      0     0      0        0       0      0   \n",
       "2       0  0      0     0      0        0       0      0   \n",
       "3       0  0      0     0      0        0       0      0   \n",
       "4       0  0      0     0      0        0       0      0   \n",
       "\n",
       "                                       text_sentence text_source  \n",
       "0                               (VOLUME, I, CHAPTER)      Austen  \n",
       "1                                                (I)      Austen  \n",
       "2  (Emma, Woodhouse, ,, handsome, ,, clever, ,, a...      Austen  \n",
       "3  (She, was, the, youngest, of, the, two, daught...      Austen  \n",
       "4  (Her, mother, had, died, too, long, ago, for, ...      Austen  \n",
       "\n",
       "[5 rows x 8215 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create our data frame with features\n",
    "word_counts = bow_features(sentences, common_words)\n",
    "word_counts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And lastly the training and holdout groups will be created. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle data before split\n",
    "word_counts = word_counts.sample(frac=1)\n",
    "# holdout\n",
    "holdout = word_counts[:int(len(word_counts)/4)]\n",
    "# redeclare train group\n",
    "word_counts = word_counts[int(len(word_counts)/4):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Means Shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Means shift found 76 clusters in the data\n"
     ]
    }
   ],
   "source": [
    "# run means shift clustering\n",
    "X = word_counts.drop(['text_sentence', 'text_source'], 1)\n",
    "from sklearn.cluster import MeanShift, estimate_bandwidth\n",
    "band = estimate_bandwidth(X, quantile=0.75)\n",
    "ms = MeanShift(bandwidth=band, bin_seeding=True)\n",
    "ms.fit(X)\n",
    "labels = ms.labels_\n",
    "cluster_centers = ms.cluster_centers_\n",
    "labels_unique = np.unique(labels)\n",
    "n_clusters_ = len(labels_unique)\n",
    "print('Means shift found {} clusters in the data'.format(len(labels_unique)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Means shift clustering seems more reliable than affinity propogation in this case becasue it can tell the model to set its own bandwith, and it does not overstimate the number of clusters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Affinity Propogation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Affinity propogation found 7638 clusters in the data\n"
     ]
    }
   ],
   "source": [
    "# run affinity propogation\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "clustering = AffinityPropagation().fit(X)\n",
    "AffinityPropagation(affinity='euclidean', convergence_iter=15, copy=True,\n",
    "          damping=0.5, max_iter=200, preference=None, verbose=False)\n",
    "print('Affinity propogation found {} clusters in the data'.format(len(clustering.labels_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Affinity propogation tends to overestimate the number of clusters in a dataset, which is true here. Also, affinity propogation works to find linkages between datapoints. Since the bow_features function produces thousands of binary columns for common words, the model likely severly overestimated the number of clusters in the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised feature generation and selection with tfidf\n",
    "\n",
    "tfidf will be used to vectorize the words in each sentence in the word_counts frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import vactorizer and set parameters\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(max_df=0.75, # drop words that occur in more than three quarters the paragraphs\n",
    "                             min_df=5, # only use words that appear at least 5 times\n",
    "                             stop_words='english', \n",
    "                             lowercase=True, #convert everything to lower case (since Alice in Wonderland has the HABIT of CAPITALIZING WORDS for EMPHASIS)\n",
    "                             use_idf=True,#we definitely want to use inverse document frequencies in our weighting\n",
    "                             norm=u'l2', #Applies a correction factor so that longer paragraphs and shorter paragraphs get treated equally\n",
    "                             smooth_idf=True #Adds 1 to all document frequencies, as if an extra document existed that used every word once.  Prevents divide-by-zero errors\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function to list tfidf scores by word by author training frame\n",
    "def get_features(author):\n",
    "    # get dataframe of words, tfidf scores and author\n",
    "    df_text = word_counts[word_counts['text_source']==author]\n",
    "    # convert text to string so tfidf works on spacy text\n",
    "    text = ','.join(str(x) for x in df_text['text_sentence'])\n",
    "    # make list for tfidf\n",
    "    text = [text]\n",
    "    # run vectorizer\n",
    "    vect = TfidfVectorizer()\n",
    "    tfidf_matrix = vect.fit_transform(text)\n",
    "    df = pd.DataFrame(tfidf_matrix.toarray(), columns = vect.get_feature_names())\n",
    "    # transpose data \n",
    "    df = df.T\n",
    "    # create response columns\n",
    "    df['author'] = '{}'.format(author)\n",
    "    df.reset_index(level=0, inplace=True)\n",
    "    df = df.rename(columns={'index': 'word', 0: 'tfidf_score'})\n",
    "    model = pd.DataFrame()\n",
    "    model = model.append(df)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>tfidf_score</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1780</td>\n",
       "      <td>0.002053</td>\n",
       "      <td>Blake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>abroad</td>\n",
       "      <td>0.002053</td>\n",
       "      <td>Blake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>abstract</td>\n",
       "      <td>0.002053</td>\n",
       "      <td>Blake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ache</td>\n",
       "      <td>0.002053</td>\n",
       "      <td>Blake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>afar</td>\n",
       "      <td>0.002053</td>\n",
       "      <td>Blake</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       word  tfidf_score author\n",
       "0      1780     0.002053  Blake\n",
       "1    abroad     0.002053  Blake\n",
       "2  abstract     0.002053  Blake\n",
       "3      ache     0.002053  Blake\n",
       "4      afar     0.002053  Blake"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get data for all authors and append dataframes\n",
    "Blake = get_features('Blake')\n",
    "Austen = get_features('Austen')\n",
    "Bryant = get_features('Bryant')\n",
    "Burgess = get_features('Burgess')\n",
    "Carroll = get_features('Carroll')\n",
    "Chesterton = get_features('Chesterton')\n",
    "Edgeworth = get_features('Edgeworth')\n",
    "Melville = get_features('Melville')\n",
    "Milton = get_features('Milton')\n",
    "Shakespeare = get_features('Shakespeare')\n",
    "# combine frames into one frame for modeling\n",
    "df_model = Blake.append([Austen, Bryant, Burgess, Carroll, Chesterton, Edgeworth, Melville, Milton, Shakespeare])\n",
    "df_model.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the word column will be used to create categorical variables for each word. This will allow the assignment of numerical values to each word that can then be used in machine learning models.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>tfidf_score</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9</td>\n",
       "      <td>0.002053</td>\n",
       "      <td>Blake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>102</td>\n",
       "      <td>0.002053</td>\n",
       "      <td>Blake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>110</td>\n",
       "      <td>0.002053</td>\n",
       "      <td>Blake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>146</td>\n",
       "      <td>0.002053</td>\n",
       "      <td>Blake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>223</td>\n",
       "      <td>0.002053</td>\n",
       "      <td>Blake</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   word  tfidf_score author\n",
       "0     9     0.002053  Blake\n",
       "1   102     0.002053  Blake\n",
       "2   110     0.002053  Blake\n",
       "3   146     0.002053  Blake\n",
       "4   223     0.002053  Blake"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert series to category dtype so cat.codes works\n",
    "df_model['word'] = df_model['word'].astype('category')\n",
    "# create dictionary of values to words in case the words need to be found again\n",
    "word_dict = dict( enumerate(df_model['word'].cat.categories) )\n",
    "# convert word column to catgegorical variables\n",
    "df_model['word'] = df_model['word'].cat.codes\n",
    "df_model.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning to predict author - H2O Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking whether there is an H2O instance running at http://localhost:54321. connected.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td>H2O cluster uptime:</td>\n",
       "<td>4 hours 8 mins</td></tr>\n",
       "<tr><td>H2O cluster timezone:</td>\n",
       "<td>America/Los_Angeles</td></tr>\n",
       "<tr><td>H2O data parsing timezone:</td>\n",
       "<td>UTC</td></tr>\n",
       "<tr><td>H2O cluster version:</td>\n",
       "<td>3.22.1.6</td></tr>\n",
       "<tr><td>H2O cluster version age:</td>\n",
       "<td>4 days </td></tr>\n",
       "<tr><td>H2O cluster name:</td>\n",
       "<td>H2O_from_python_kyleknoebel_g50r14</td></tr>\n",
       "<tr><td>H2O cluster total nodes:</td>\n",
       "<td>1</td></tr>\n",
       "<tr><td>H2O cluster free memory:</td>\n",
       "<td>1.261 Gb</td></tr>\n",
       "<tr><td>H2O cluster total cores:</td>\n",
       "<td>4</td></tr>\n",
       "<tr><td>H2O cluster allowed cores:</td>\n",
       "<td>4</td></tr>\n",
       "<tr><td>H2O cluster status:</td>\n",
       "<td>locked, healthy</td></tr>\n",
       "<tr><td>H2O connection url:</td>\n",
       "<td>http://localhost:54321</td></tr>\n",
       "<tr><td>H2O connection proxy:</td>\n",
       "<td>None</td></tr>\n",
       "<tr><td>H2O internal security:</td>\n",
       "<td>False</td></tr>\n",
       "<tr><td>H2O API Extensions:</td>\n",
       "<td>Amazon S3, XGBoost, Algos, AutoML, Core V3, Core V4</td></tr>\n",
       "<tr><td>Python version:</td>\n",
       "<td>3.6.8 final</td></tr></table></div>"
      ],
      "text/plain": [
       "--------------------------  ---------------------------------------------------\n",
       "H2O cluster uptime:         4 hours 8 mins\n",
       "H2O cluster timezone:       America/Los_Angeles\n",
       "H2O data parsing timezone:  UTC\n",
       "H2O cluster version:        3.22.1.6\n",
       "H2O cluster version age:    4 days\n",
       "H2O cluster name:           H2O_from_python_kyleknoebel_g50r14\n",
       "H2O cluster total nodes:    1\n",
       "H2O cluster free memory:    1.261 Gb\n",
       "H2O cluster total cores:    4\n",
       "H2O cluster allowed cores:  4\n",
       "H2O cluster status:         locked, healthy\n",
       "H2O connection url:         http://localhost:54321\n",
       "H2O connection proxy:\n",
       "H2O internal security:      False\n",
       "H2O API Extensions:         Amazon S3, XGBoost, Algos, AutoML, Core V3, Core V4\n",
       "Python version:             3.6.8 final\n",
       "--------------------------  ---------------------------------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import h2o\n",
    "h2o.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised - Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n",
      "deeplearning Model Build progress: |██████████████████████████████████████| 100%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>duration</th>\n",
       "      <th>training_speed</th>\n",
       "      <th>epochs</th>\n",
       "      <th>iterations</th>\n",
       "      <th>samples</th>\n",
       "      <th>training_rmse</th>\n",
       "      <th>training_logloss</th>\n",
       "      <th>training_r2</th>\n",
       "      <th>training_classification_error</th>\n",
       "      <th>validation_rmse</th>\n",
       "      <th>validation_logloss</th>\n",
       "      <th>validation_r2</th>\n",
       "      <th>validation_classification_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td>2019-03-18 20:02:35</td>\n",
       "      <td>0.000 sec</td>\n",
       "      <td>None</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td></td>\n",
       "      <td>2019-03-18 20:02:35</td>\n",
       "      <td>0.220 sec</td>\n",
       "      <td>719690 obs/sec</td>\n",
       "      <td>6.060645</td>\n",
       "      <td>1</td>\n",
       "      <td>100037.0</td>\n",
       "      <td>0.888103</td>\n",
       "      <td>2.236034</td>\n",
       "      <td>0.903265</td>\n",
       "      <td>0.844671</td>\n",
       "      <td>0.888952</td>\n",
       "      <td>2.244737</td>\n",
       "      <td>0.902019</td>\n",
       "      <td>0.845402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "      <td>2019-03-18 20:02:36</td>\n",
       "      <td>1.950 sec</td>\n",
       "      <td>921016 obs/sec</td>\n",
       "      <td>102.948928</td>\n",
       "      <td>17</td>\n",
       "      <td>1699275.0</td>\n",
       "      <td>0.882517</td>\n",
       "      <td>2.182736</td>\n",
       "      <td>0.904478</td>\n",
       "      <td>0.844671</td>\n",
       "      <td>0.883044</td>\n",
       "      <td>2.187973</td>\n",
       "      <td>0.903317</td>\n",
       "      <td>0.845402</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               timestamp    duration  training_speed      epochs  iterations  \\\n",
       "0    2019-03-18 20:02:35   0.000 sec            None    0.000000           0   \n",
       "1    2019-03-18 20:02:35   0.220 sec  719690 obs/sec    6.060645           1   \n",
       "2    2019-03-18 20:02:36   1.950 sec  921016 obs/sec  102.948928          17   \n",
       "\n",
       "     samples  training_rmse  training_logloss  training_r2  \\\n",
       "0        0.0            NaN               NaN          NaN   \n",
       "1   100037.0       0.888103          2.236034     0.903265   \n",
       "2  1699275.0       0.882517          2.182736     0.904478   \n",
       "\n",
       "   training_classification_error  validation_rmse  validation_logloss  \\\n",
       "0                            NaN              NaN                 NaN   \n",
       "1                       0.844671         0.888952            2.244737   \n",
       "2                       0.844671         0.883044            2.187973   \n",
       "\n",
       "   validation_r2  validation_classification_error  \n",
       "0            NaN                              NaN  \n",
       "1       0.902019                         0.845402  \n",
       "2       0.903317                         0.845402  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shuffle data \n",
    "df_model = df_model.sample(frac=1)\n",
    "# create dataframe for H2O\n",
    "df_h2o_model= h2o.H2OFrame(df_model)\n",
    "# create training, test, validation frames\n",
    "train,test,valid = df_h2o_model.split_frame(ratios = [0.75, 0.15])\n",
    "# import deep learning estimator\n",
    "from h2o.estimators.deeplearning import H2ODeepLearningEstimator\n",
    "# create response and training variables\n",
    "x = ['word', 'tfidf_score']\n",
    "y = 'author'\n",
    "# deep learning parameters\n",
    "nn_model = H2ODeepLearningEstimator(hidden=[2, 2], epochs=100, seed=1234)\n",
    "nn_model.train(x=x, y=y, training_frame=train, validation_frame=valid)\n",
    "# show accuracy\n",
    "nn_model._model_json['output']['scoring_history'].as_data_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised - Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drf Model Build progress: |███████████████████████████████████████████████| 100%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>duration</th>\n",
       "      <th>number_of_trees</th>\n",
       "      <th>training_rmse</th>\n",
       "      <th>training_logloss</th>\n",
       "      <th>training_classification_error</th>\n",
       "      <th>validation_rmse</th>\n",
       "      <th>validation_logloss</th>\n",
       "      <th>validation_classification_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td>2019-03-18 20:02:37</td>\n",
       "      <td>0.003 sec</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td></td>\n",
       "      <td>2019-03-18 20:02:37</td>\n",
       "      <td>0.106 sec</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.513570</td>\n",
       "      <td>7.668502</td>\n",
       "      <td>0.271107</td>\n",
       "      <td>0.529597</td>\n",
       "      <td>8.038819</td>\n",
       "      <td>0.302532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "      <td>2019-03-18 20:02:37</td>\n",
       "      <td>0.167 sec</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.438099</td>\n",
       "      <td>5.463474</td>\n",
       "      <td>0.196340</td>\n",
       "      <td>0.385955</td>\n",
       "      <td>3.431103</td>\n",
       "      <td>0.152821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td></td>\n",
       "      <td>2019-03-18 20:02:37</td>\n",
       "      <td>0.234 sec</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.404718</td>\n",
       "      <td>4.500523</td>\n",
       "      <td>0.167690</td>\n",
       "      <td>0.354070</td>\n",
       "      <td>2.297694</td>\n",
       "      <td>0.126166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td></td>\n",
       "      <td>2019-03-18 20:02:37</td>\n",
       "      <td>0.311 sec</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.400518</td>\n",
       "      <td>4.226616</td>\n",
       "      <td>0.163716</td>\n",
       "      <td>0.356834</td>\n",
       "      <td>2.065236</td>\n",
       "      <td>0.126610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td></td>\n",
       "      <td>2019-03-18 20:02:37</td>\n",
       "      <td>0.389 sec</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.397720</td>\n",
       "      <td>3.984169</td>\n",
       "      <td>0.162863</td>\n",
       "      <td>0.356790</td>\n",
       "      <td>1.882607</td>\n",
       "      <td>0.124389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td></td>\n",
       "      <td>2019-03-18 20:02:37</td>\n",
       "      <td>0.470 sec</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.383646</td>\n",
       "      <td>3.524920</td>\n",
       "      <td>0.151394</td>\n",
       "      <td>0.347686</td>\n",
       "      <td>1.641514</td>\n",
       "      <td>0.113727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td></td>\n",
       "      <td>2019-03-18 20:02:37</td>\n",
       "      <td>0.554 sec</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.373304</td>\n",
       "      <td>3.184186</td>\n",
       "      <td>0.141696</td>\n",
       "      <td>0.342813</td>\n",
       "      <td>1.478276</td>\n",
       "      <td>0.108841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td></td>\n",
       "      <td>2019-03-18 20:02:37</td>\n",
       "      <td>0.649 sec</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.369670</td>\n",
       "      <td>2.973683</td>\n",
       "      <td>0.138407</td>\n",
       "      <td>0.342691</td>\n",
       "      <td>1.394646</td>\n",
       "      <td>0.107952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td></td>\n",
       "      <td>2019-03-18 20:02:38</td>\n",
       "      <td>0.745 sec</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.358817</td>\n",
       "      <td>2.660778</td>\n",
       "      <td>0.130153</td>\n",
       "      <td>0.332994</td>\n",
       "      <td>1.273919</td>\n",
       "      <td>0.103065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td></td>\n",
       "      <td>2019-03-18 20:02:38</td>\n",
       "      <td>0.850 sec</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.352828</td>\n",
       "      <td>2.431907</td>\n",
       "      <td>0.124916</td>\n",
       "      <td>0.330206</td>\n",
       "      <td>1.159652</td>\n",
       "      <td>0.099511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td></td>\n",
       "      <td>2019-03-18 20:02:38</td>\n",
       "      <td>0.962 sec</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.346222</td>\n",
       "      <td>2.238428</td>\n",
       "      <td>0.118927</td>\n",
       "      <td>0.326603</td>\n",
       "      <td>1.123703</td>\n",
       "      <td>0.095069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td></td>\n",
       "      <td>2019-03-18 20:02:38</td>\n",
       "      <td>1.081 sec</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.345413</td>\n",
       "      <td>2.126062</td>\n",
       "      <td>0.117937</td>\n",
       "      <td>0.325617</td>\n",
       "      <td>0.982607</td>\n",
       "      <td>0.095069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td></td>\n",
       "      <td>2019-03-18 20:02:38</td>\n",
       "      <td>1.209 sec</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.344569</td>\n",
       "      <td>2.025684</td>\n",
       "      <td>0.115922</td>\n",
       "      <td>0.327390</td>\n",
       "      <td>0.973378</td>\n",
       "      <td>0.095069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td></td>\n",
       "      <td>2019-03-18 20:02:38</td>\n",
       "      <td>1.346 sec</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.343104</td>\n",
       "      <td>1.945387</td>\n",
       "      <td>0.114738</td>\n",
       "      <td>0.328000</td>\n",
       "      <td>0.905684</td>\n",
       "      <td>0.097290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td></td>\n",
       "      <td>2019-03-18 20:02:38</td>\n",
       "      <td>1.478 sec</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.340047</td>\n",
       "      <td>1.839245</td>\n",
       "      <td>0.112796</td>\n",
       "      <td>0.325758</td>\n",
       "      <td>0.832215</td>\n",
       "      <td>0.097734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td></td>\n",
       "      <td>2019-03-18 20:02:38</td>\n",
       "      <td>1.622 sec</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.339581</td>\n",
       "      <td>1.766672</td>\n",
       "      <td>0.111946</td>\n",
       "      <td>0.326367</td>\n",
       "      <td>0.820730</td>\n",
       "      <td>0.096402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td></td>\n",
       "      <td>2019-03-18 20:02:39</td>\n",
       "      <td>1.769 sec</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.338886</td>\n",
       "      <td>1.700378</td>\n",
       "      <td>0.111555</td>\n",
       "      <td>0.325743</td>\n",
       "      <td>0.791307</td>\n",
       "      <td>0.095957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td></td>\n",
       "      <td>2019-03-18 20:02:39</td>\n",
       "      <td>1.933 sec</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.340862</td>\n",
       "      <td>1.637370</td>\n",
       "      <td>0.111003</td>\n",
       "      <td>0.328259</td>\n",
       "      <td>0.770915</td>\n",
       "      <td>0.096846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td></td>\n",
       "      <td>2019-03-18 20:02:39</td>\n",
       "      <td>2.106 sec</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.343292</td>\n",
       "      <td>1.601866</td>\n",
       "      <td>0.111724</td>\n",
       "      <td>0.330305</td>\n",
       "      <td>0.763126</td>\n",
       "      <td>0.096402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td></td>\n",
       "      <td>2019-03-18 20:02:39</td>\n",
       "      <td>2.280 sec</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.342667</td>\n",
       "      <td>1.551153</td>\n",
       "      <td>0.111118</td>\n",
       "      <td>0.330469</td>\n",
       "      <td>0.751068</td>\n",
       "      <td>0.095957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td></td>\n",
       "      <td>2019-03-18 20:02:39</td>\n",
       "      <td>2.460 sec</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0.341898</td>\n",
       "      <td>1.511837</td>\n",
       "      <td>0.110081</td>\n",
       "      <td>0.329514</td>\n",
       "      <td>0.707182</td>\n",
       "      <td>0.093292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td></td>\n",
       "      <td>2019-03-18 20:02:39</td>\n",
       "      <td>2.640 sec</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.339657</td>\n",
       "      <td>1.464394</td>\n",
       "      <td>0.108991</td>\n",
       "      <td>0.327846</td>\n",
       "      <td>0.702129</td>\n",
       "      <td>0.091959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td></td>\n",
       "      <td>2019-03-18 20:02:40</td>\n",
       "      <td>2.834 sec</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.336199</td>\n",
       "      <td>1.385097</td>\n",
       "      <td>0.106264</td>\n",
       "      <td>0.324634</td>\n",
       "      <td>0.653880</td>\n",
       "      <td>0.089738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td></td>\n",
       "      <td>2019-03-18 20:02:40</td>\n",
       "      <td>3.033 sec</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.338199</td>\n",
       "      <td>1.362551</td>\n",
       "      <td>0.107658</td>\n",
       "      <td>0.326827</td>\n",
       "      <td>0.659526</td>\n",
       "      <td>0.091959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td></td>\n",
       "      <td>2019-03-18 20:02:40</td>\n",
       "      <td>3.241 sec</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.339577</td>\n",
       "      <td>1.345902</td>\n",
       "      <td>0.107840</td>\n",
       "      <td>0.328571</td>\n",
       "      <td>0.663865</td>\n",
       "      <td>0.091071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td></td>\n",
       "      <td>2019-03-18 20:02:40</td>\n",
       "      <td>3.462 sec</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.341028</td>\n",
       "      <td>1.324844</td>\n",
       "      <td>0.107900</td>\n",
       "      <td>0.330653</td>\n",
       "      <td>0.654537</td>\n",
       "      <td>0.092848</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                timestamp    duration  number_of_trees  training_rmse  \\\n",
       "0     2019-03-18 20:02:37   0.003 sec              0.0            NaN   \n",
       "1     2019-03-18 20:02:37   0.106 sec              1.0       0.513570   \n",
       "2     2019-03-18 20:02:37   0.167 sec              2.0       0.438099   \n",
       "3     2019-03-18 20:02:37   0.234 sec              3.0       0.404718   \n",
       "4     2019-03-18 20:02:37   0.311 sec              4.0       0.400518   \n",
       "5     2019-03-18 20:02:37   0.389 sec              5.0       0.397720   \n",
       "6     2019-03-18 20:02:37   0.470 sec              6.0       0.383646   \n",
       "7     2019-03-18 20:02:37   0.554 sec              7.0       0.373304   \n",
       "8     2019-03-18 20:02:37   0.649 sec              8.0       0.369670   \n",
       "9     2019-03-18 20:02:38   0.745 sec              9.0       0.358817   \n",
       "10    2019-03-18 20:02:38   0.850 sec             10.0       0.352828   \n",
       "11    2019-03-18 20:02:38   0.962 sec             11.0       0.346222   \n",
       "12    2019-03-18 20:02:38   1.081 sec             12.0       0.345413   \n",
       "13    2019-03-18 20:02:38   1.209 sec             13.0       0.344569   \n",
       "14    2019-03-18 20:02:38   1.346 sec             14.0       0.343104   \n",
       "15    2019-03-18 20:02:38   1.478 sec             15.0       0.340047   \n",
       "16    2019-03-18 20:02:38   1.622 sec             16.0       0.339581   \n",
       "17    2019-03-18 20:02:39   1.769 sec             17.0       0.338886   \n",
       "18    2019-03-18 20:02:39   1.933 sec             18.0       0.340862   \n",
       "19    2019-03-18 20:02:39   2.106 sec             19.0       0.343292   \n",
       "20    2019-03-18 20:02:39   2.280 sec             20.0       0.342667   \n",
       "21    2019-03-18 20:02:39   2.460 sec             21.0       0.341898   \n",
       "22    2019-03-18 20:02:39   2.640 sec             22.0       0.339657   \n",
       "23    2019-03-18 20:02:40   2.834 sec             23.0       0.336199   \n",
       "24    2019-03-18 20:02:40   3.033 sec             24.0       0.338199   \n",
       "25    2019-03-18 20:02:40   3.241 sec             25.0       0.339577   \n",
       "26    2019-03-18 20:02:40   3.462 sec             26.0       0.341028   \n",
       "\n",
       "    training_logloss  training_classification_error  validation_rmse  \\\n",
       "0                NaN                            NaN              NaN   \n",
       "1           7.668502                       0.271107         0.529597   \n",
       "2           5.463474                       0.196340         0.385955   \n",
       "3           4.500523                       0.167690         0.354070   \n",
       "4           4.226616                       0.163716         0.356834   \n",
       "5           3.984169                       0.162863         0.356790   \n",
       "6           3.524920                       0.151394         0.347686   \n",
       "7           3.184186                       0.141696         0.342813   \n",
       "8           2.973683                       0.138407         0.342691   \n",
       "9           2.660778                       0.130153         0.332994   \n",
       "10          2.431907                       0.124916         0.330206   \n",
       "11          2.238428                       0.118927         0.326603   \n",
       "12          2.126062                       0.117937         0.325617   \n",
       "13          2.025684                       0.115922         0.327390   \n",
       "14          1.945387                       0.114738         0.328000   \n",
       "15          1.839245                       0.112796         0.325758   \n",
       "16          1.766672                       0.111946         0.326367   \n",
       "17          1.700378                       0.111555         0.325743   \n",
       "18          1.637370                       0.111003         0.328259   \n",
       "19          1.601866                       0.111724         0.330305   \n",
       "20          1.551153                       0.111118         0.330469   \n",
       "21          1.511837                       0.110081         0.329514   \n",
       "22          1.464394                       0.108991         0.327846   \n",
       "23          1.385097                       0.106264         0.324634   \n",
       "24          1.362551                       0.107658         0.326827   \n",
       "25          1.345902                       0.107840         0.328571   \n",
       "26          1.324844                       0.107900         0.330653   \n",
       "\n",
       "    validation_logloss  validation_classification_error  \n",
       "0                  NaN                              NaN  \n",
       "1             8.038819                         0.302532  \n",
       "2             3.431103                         0.152821  \n",
       "3             2.297694                         0.126166  \n",
       "4             2.065236                         0.126610  \n",
       "5             1.882607                         0.124389  \n",
       "6             1.641514                         0.113727  \n",
       "7             1.478276                         0.108841  \n",
       "8             1.394646                         0.107952  \n",
       "9             1.273919                         0.103065  \n",
       "10            1.159652                         0.099511  \n",
       "11            1.123703                         0.095069  \n",
       "12            0.982607                         0.095069  \n",
       "13            0.973378                         0.095069  \n",
       "14            0.905684                         0.097290  \n",
       "15            0.832215                         0.097734  \n",
       "16            0.820730                         0.096402  \n",
       "17            0.791307                         0.095957  \n",
       "18            0.770915                         0.096846  \n",
       "19            0.763126                         0.096402  \n",
       "20            0.751068                         0.095957  \n",
       "21            0.707182                         0.093292  \n",
       "22            0.702129                         0.091959  \n",
       "23            0.653880                         0.089738  \n",
       "24            0.659526                         0.091959  \n",
       "25            0.663865                         0.091071  \n",
       "26            0.654537                         0.092848  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import model\n",
    "from h2o.estimators.random_forest import H2ORandomForestEstimator\n",
    "# deep learning parameters\n",
    "rf_model = H2ORandomForestEstimator(ntrees=200, stopping_rounds=2, seed=1234)\n",
    "rf_model.train(x=x, y=y, training_frame=train, validation_frame=valid)\n",
    "# show accuracy\n",
    "rf_model._model_json['output']['scoring_history'].as_data_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Holdout testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>tfidf_score</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>about</td>\n",
       "      <td>0.005308</td>\n",
       "      <td>Blake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>admired</td>\n",
       "      <td>0.005308</td>\n",
       "      <td>Blake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>adona</td>\n",
       "      <td>0.005308</td>\n",
       "      <td>Blake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>affright</td>\n",
       "      <td>0.005308</td>\n",
       "      <td>Blake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>against</td>\n",
       "      <td>0.005308</td>\n",
       "      <td>Blake</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       word  tfidf_score author\n",
       "0     about     0.005308  Blake\n",
       "1   admired     0.005308  Blake\n",
       "2     adona     0.005308  Blake\n",
       "3  affright     0.005308  Blake\n",
       "4   against     0.005308  Blake"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create function for holdout group \n",
    "def get_features(author):\n",
    "    # get dataframe of words, tfidf scores and author\n",
    "    df_text = holdout[holdout['text_source']==author]\n",
    "    text = ','.join(str(x) for x in df_text['text_sentence'])\n",
    "    text = [text]\n",
    "    vect = TfidfVectorizer()\n",
    "    tfidf_matrix = vect.fit_transform(text)\n",
    "    df = pd.DataFrame(tfidf_matrix.toarray(), columns = vect.get_feature_names())\n",
    "    df = df.T\n",
    "    df['author'] = '{}'.format(author)\n",
    "    df.reset_index(level=0, inplace=True)\n",
    "    df = df.rename(columns={'index': 'word', 0: 'tfidf_score'})\n",
    "    model = pd.DataFrame()\n",
    "    model = model.append(df)\n",
    "    return model\n",
    "# get data for all authors and append dataframes\n",
    "Blake = get_features('Blake')\n",
    "Austen = get_features('Austen')\n",
    "Bryant = get_features('Bryant')\n",
    "Burgess = get_features('Burgess')\n",
    "Carroll = get_features('Carroll')\n",
    "Chesterton = get_features('Chesterton')\n",
    "Edgeworth = get_features('Edgeworth')\n",
    "Melville = get_features('Melville')\n",
    "Milton = get_features('Milton')\n",
    "Shakespeare = get_features('Shakespeare')\n",
    "df_holdout = Blake.append([Austen, Bryant, Burgess, Carroll, Chesterton, Edgeworth, Melville, Milton, Shakespeare])\n",
    "df_holdout.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Means shift found 22 clusters in the data\n"
     ]
    }
   ],
   "source": [
    "# run means shift on holdout frame\n",
    "X = holdout.drop(['text_sentence', 'text_source'], 1)\n",
    "from sklearn.cluster import MeanShift, estimate_bandwidth\n",
    "band = estimate_bandwidth(X, quantile=0.75)\n",
    "ms = MeanShift(bandwidth=band, bin_seeding=True)\n",
    "ms.fit(X)\n",
    "labels = ms.labels_\n",
    "cluster_centers = ms.cluster_centers_\n",
    "labels_unique = np.unique(labels)\n",
    "n_clusters_ = len(labels_unique)\n",
    "print('Means shift found {} clusters in the data'.format(len(labels_unique)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Affinity propogation found 2546 clusters in the data\n"
     ]
    }
   ],
   "source": [
    "# run affinity propogation\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "clustering = AffinityPropagation().fit(X)\n",
    "AffinityPropagation(affinity='euclidean', convergence_iter=15, copy=True,\n",
    "          damping=0.5, max_iter=200, preference=None, verbose=False)\n",
    "print('Affinity propogation found {} clusters in the data'.format(len(clustering.labels_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n",
      "deeplearning Model Build progress: |██████████████████████████████████████| 100%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>duration</th>\n",
       "      <th>training_speed</th>\n",
       "      <th>epochs</th>\n",
       "      <th>iterations</th>\n",
       "      <th>samples</th>\n",
       "      <th>training_rmse</th>\n",
       "      <th>training_logloss</th>\n",
       "      <th>training_r2</th>\n",
       "      <th>training_classification_error</th>\n",
       "      <th>validation_rmse</th>\n",
       "      <th>validation_logloss</th>\n",
       "      <th>validation_r2</th>\n",
       "      <th>validation_classification_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td>2019-03-18 20:03:09</td>\n",
       "      <td>0.000 sec</td>\n",
       "      <td>None</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td></td>\n",
       "      <td>2019-03-18 20:03:09</td>\n",
       "      <td>0.130 sec</td>\n",
       "      <td>192222 obs/sec</td>\n",
       "      <td>0.401578</td>\n",
       "      <td>1</td>\n",
       "      <td>3460.0</td>\n",
       "      <td>0.887166</td>\n",
       "      <td>2.569062</td>\n",
       "      <td>0.903228</td>\n",
       "      <td>0.860608</td>\n",
       "      <td>0.887862</td>\n",
       "      <td>2.582324</td>\n",
       "      <td>0.901148</td>\n",
       "      <td>0.859867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "      <td>2019-03-18 20:03:14</td>\n",
       "      <td>5.147 sec</td>\n",
       "      <td>136454 obs/sec</td>\n",
       "      <td>78.236305</td>\n",
       "      <td>191</td>\n",
       "      <td>674084.0</td>\n",
       "      <td>0.894950</td>\n",
       "      <td>2.392631</td>\n",
       "      <td>0.901522</td>\n",
       "      <td>0.859448</td>\n",
       "      <td>0.898833</td>\n",
       "      <td>2.422272</td>\n",
       "      <td>0.898690</td>\n",
       "      <td>0.878109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td></td>\n",
       "      <td>2019-03-18 20:03:15</td>\n",
       "      <td>6.729 sec</td>\n",
       "      <td>134690 obs/sec</td>\n",
       "      <td>100.408310</td>\n",
       "      <td>245</td>\n",
       "      <td>865118.0</td>\n",
       "      <td>0.896959</td>\n",
       "      <td>2.433828</td>\n",
       "      <td>0.901079</td>\n",
       "      <td>0.908890</td>\n",
       "      <td>0.894628</td>\n",
       "      <td>2.407989</td>\n",
       "      <td>0.899635</td>\n",
       "      <td>0.903814</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               timestamp    duration  training_speed      epochs  iterations  \\\n",
       "0    2019-03-18 20:03:09   0.000 sec            None    0.000000           0   \n",
       "1    2019-03-18 20:03:09   0.130 sec  192222 obs/sec    0.401578           1   \n",
       "2    2019-03-18 20:03:14   5.147 sec  136454 obs/sec   78.236305         191   \n",
       "3    2019-03-18 20:03:15   6.729 sec  134690 obs/sec  100.408310         245   \n",
       "\n",
       "    samples  training_rmse  training_logloss  training_r2  \\\n",
       "0       0.0            NaN               NaN          NaN   \n",
       "1    3460.0       0.887166          2.569062     0.903228   \n",
       "2  674084.0       0.894950          2.392631     0.901522   \n",
       "3  865118.0       0.896959          2.433828     0.901079   \n",
       "\n",
       "   training_classification_error  validation_rmse  validation_logloss  \\\n",
       "0                            NaN              NaN                 NaN   \n",
       "1                       0.860608         0.887862            2.582324   \n",
       "2                       0.859448         0.898833            2.422272   \n",
       "3                       0.908890         0.894628            2.407989   \n",
       "\n",
       "   validation_r2  validation_classification_error  \n",
       "0            NaN                              NaN  \n",
       "1       0.901148                         0.859867  \n",
       "2       0.898690                         0.878109  \n",
       "3       0.899635                         0.903814  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shuffle data \n",
    "df_holdout = df_holdout.sample(frac=1)\n",
    "# create dataframe for H2O\n",
    "df_h2o_model_holdout= h2o.H2OFrame(df_holdout)\n",
    "# create training, test, validation frames\n",
    "train_h,test_h,valid_h = df_h2o_model_holdout.split_frame(ratios = [0.75, 0.15])\n",
    "# create response and training variables\n",
    "x = ['word', 'tfidf_score']\n",
    "y = 'author'\n",
    "# deep learning parameters\n",
    "nn_model_h = H2ODeepLearningEstimator(hidden=[2, 2], epochs=100, seed=1234)\n",
    "nn_model_h.train(x=x, y=y, training_frame=train_h, validation_frame=valid_h)\n",
    "# show accuracy\n",
    "nn_model_h._model_json['output']['scoring_history'].as_data_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drf Model Build progress: |███████████████████████████████████████████████| 100%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>duration</th>\n",
       "      <th>number_of_trees</th>\n",
       "      <th>training_rmse</th>\n",
       "      <th>training_logloss</th>\n",
       "      <th>training_classification_error</th>\n",
       "      <th>validation_rmse</th>\n",
       "      <th>validation_logloss</th>\n",
       "      <th>validation_classification_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td>2019-03-18 20:03:16</td>\n",
       "      <td>0.023 sec</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td></td>\n",
       "      <td>2019-03-18 20:03:16</td>\n",
       "      <td>0.117 sec</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.612213</td>\n",
       "      <td>12.228297</td>\n",
       "      <td>0.355035</td>\n",
       "      <td>0.626379</td>\n",
       "      <td>12.680359</td>\n",
       "      <td>0.397181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "      <td>2019-03-18 20:03:16</td>\n",
       "      <td>0.154 sec</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.506486</td>\n",
       "      <td>8.307597</td>\n",
       "      <td>0.250291</td>\n",
       "      <td>0.405501</td>\n",
       "      <td>4.678126</td>\n",
       "      <td>0.170813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td></td>\n",
       "      <td>2019-03-18 20:03:16</td>\n",
       "      <td>0.191 sec</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.441045</td>\n",
       "      <td>6.074987</td>\n",
       "      <td>0.193444</td>\n",
       "      <td>0.339030</td>\n",
       "      <td>2.649073</td>\n",
       "      <td>0.120232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td></td>\n",
       "      <td>2019-03-18 20:03:16</td>\n",
       "      <td>0.245 sec</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.421699</td>\n",
       "      <td>5.335349</td>\n",
       "      <td>0.178799</td>\n",
       "      <td>0.336118</td>\n",
       "      <td>2.256579</td>\n",
       "      <td>0.116086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td></td>\n",
       "      <td>2019-03-18 20:03:16</td>\n",
       "      <td>0.318 sec</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.418647</td>\n",
       "      <td>5.102575</td>\n",
       "      <td>0.177760</td>\n",
       "      <td>0.337905</td>\n",
       "      <td>2.027977</td>\n",
       "      <td>0.117745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td></td>\n",
       "      <td>2019-03-18 20:03:17</td>\n",
       "      <td>0.376 sec</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.397057</td>\n",
       "      <td>4.372776</td>\n",
       "      <td>0.158025</td>\n",
       "      <td>0.329788</td>\n",
       "      <td>1.706817</td>\n",
       "      <td>0.109453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td></td>\n",
       "      <td>2019-03-18 20:03:17</td>\n",
       "      <td>0.427 sec</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.377551</td>\n",
       "      <td>3.718919</td>\n",
       "      <td>0.143737</td>\n",
       "      <td>0.320569</td>\n",
       "      <td>1.452873</td>\n",
       "      <td>0.102819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td></td>\n",
       "      <td>2019-03-18 20:03:17</td>\n",
       "      <td>0.484 sec</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.369709</td>\n",
       "      <td>3.451601</td>\n",
       "      <td>0.137274</td>\n",
       "      <td>0.322780</td>\n",
       "      <td>1.429145</td>\n",
       "      <td>0.101990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td></td>\n",
       "      <td>2019-03-18 20:03:17</td>\n",
       "      <td>0.540 sec</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.356990</td>\n",
       "      <td>3.065563</td>\n",
       "      <td>0.127815</td>\n",
       "      <td>0.318121</td>\n",
       "      <td>1.318139</td>\n",
       "      <td>0.097844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td></td>\n",
       "      <td>2019-03-18 20:03:17</td>\n",
       "      <td>0.600 sec</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.345275</td>\n",
       "      <td>2.742843</td>\n",
       "      <td>0.118047</td>\n",
       "      <td>0.311870</td>\n",
       "      <td>1.202616</td>\n",
       "      <td>0.090381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td></td>\n",
       "      <td>2019-03-18 20:03:17</td>\n",
       "      <td>0.663 sec</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.335225</td>\n",
       "      <td>2.446840</td>\n",
       "      <td>0.111345</td>\n",
       "      <td>0.307735</td>\n",
       "      <td>1.091966</td>\n",
       "      <td>0.093698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td></td>\n",
       "      <td>2019-03-18 20:03:17</td>\n",
       "      <td>0.734 sec</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.332255</td>\n",
       "      <td>2.334162</td>\n",
       "      <td>0.109248</td>\n",
       "      <td>0.307513</td>\n",
       "      <td>1.063560</td>\n",
       "      <td>0.092869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td></td>\n",
       "      <td>2019-03-18 20:03:17</td>\n",
       "      <td>0.808 sec</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.331825</td>\n",
       "      <td>2.210141</td>\n",
       "      <td>0.108708</td>\n",
       "      <td>0.306161</td>\n",
       "      <td>0.979710</td>\n",
       "      <td>0.086235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td></td>\n",
       "      <td>2019-03-18 20:03:17</td>\n",
       "      <td>0.882 sec</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.328639</td>\n",
       "      <td>2.110953</td>\n",
       "      <td>0.106657</td>\n",
       "      <td>0.306540</td>\n",
       "      <td>0.980853</td>\n",
       "      <td>0.087894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td></td>\n",
       "      <td>2019-03-18 20:03:17</td>\n",
       "      <td>0.956 sec</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.325734</td>\n",
       "      <td>1.995089</td>\n",
       "      <td>0.103949</td>\n",
       "      <td>0.304762</td>\n",
       "      <td>0.950863</td>\n",
       "      <td>0.088723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td></td>\n",
       "      <td>2019-03-18 20:03:17</td>\n",
       "      <td>1.043 sec</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.325565</td>\n",
       "      <td>1.908639</td>\n",
       "      <td>0.103564</td>\n",
       "      <td>0.305527</td>\n",
       "      <td>0.851402</td>\n",
       "      <td>0.088723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td></td>\n",
       "      <td>2019-03-18 20:03:17</td>\n",
       "      <td>1.127 sec</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.323582</td>\n",
       "      <td>1.803415</td>\n",
       "      <td>0.102159</td>\n",
       "      <td>0.305310</td>\n",
       "      <td>0.851793</td>\n",
       "      <td>0.092040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td></td>\n",
       "      <td>2019-03-18 20:03:17</td>\n",
       "      <td>1.226 sec</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.325102</td>\n",
       "      <td>1.766299</td>\n",
       "      <td>0.103076</td>\n",
       "      <td>0.308479</td>\n",
       "      <td>0.807913</td>\n",
       "      <td>0.094527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td></td>\n",
       "      <td>2019-03-18 20:03:17</td>\n",
       "      <td>1.328 sec</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.328760</td>\n",
       "      <td>1.753563</td>\n",
       "      <td>0.104457</td>\n",
       "      <td>0.312366</td>\n",
       "      <td>0.817136</td>\n",
       "      <td>0.093698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td></td>\n",
       "      <td>2019-03-18 20:03:18</td>\n",
       "      <td>1.423 sec</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.327908</td>\n",
       "      <td>1.690811</td>\n",
       "      <td>0.104457</td>\n",
       "      <td>0.312430</td>\n",
       "      <td>0.768916</td>\n",
       "      <td>0.093698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td></td>\n",
       "      <td>2019-03-18 20:03:18</td>\n",
       "      <td>1.522 sec</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0.326994</td>\n",
       "      <td>1.631042</td>\n",
       "      <td>0.103644</td>\n",
       "      <td>0.312311</td>\n",
       "      <td>0.769718</td>\n",
       "      <td>0.092869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td></td>\n",
       "      <td>2019-03-18 20:03:18</td>\n",
       "      <td>1.624 sec</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.326306</td>\n",
       "      <td>1.586581</td>\n",
       "      <td>0.102948</td>\n",
       "      <td>0.311774</td>\n",
       "      <td>0.768383</td>\n",
       "      <td>0.092869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td></td>\n",
       "      <td>2019-03-18 20:03:18</td>\n",
       "      <td>1.727 sec</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.320351</td>\n",
       "      <td>1.490570</td>\n",
       "      <td>0.098654</td>\n",
       "      <td>0.307810</td>\n",
       "      <td>0.734756</td>\n",
       "      <td>0.092040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td></td>\n",
       "      <td>2019-03-18 20:03:18</td>\n",
       "      <td>1.846 sec</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.322154</td>\n",
       "      <td>1.463251</td>\n",
       "      <td>0.099234</td>\n",
       "      <td>0.310267</td>\n",
       "      <td>0.740826</td>\n",
       "      <td>0.092869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td></td>\n",
       "      <td>2019-03-18 20:03:18</td>\n",
       "      <td>1.963 sec</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.322747</td>\n",
       "      <td>1.428161</td>\n",
       "      <td>0.099466</td>\n",
       "      <td>0.311251</td>\n",
       "      <td>0.715792</td>\n",
       "      <td>0.092869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td></td>\n",
       "      <td>2019-03-18 20:03:18</td>\n",
       "      <td>2.085 sec</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.324526</td>\n",
       "      <td>1.415446</td>\n",
       "      <td>0.099234</td>\n",
       "      <td>0.313407</td>\n",
       "      <td>0.721587</td>\n",
       "      <td>0.092869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td></td>\n",
       "      <td>2019-03-18 20:03:18</td>\n",
       "      <td>2.208 sec</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.324279</td>\n",
       "      <td>1.407645</td>\n",
       "      <td>0.099582</td>\n",
       "      <td>0.313540</td>\n",
       "      <td>0.695037</td>\n",
       "      <td>0.092040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td></td>\n",
       "      <td>2019-03-18 20:03:19</td>\n",
       "      <td>2.385 sec</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.325019</td>\n",
       "      <td>1.373881</td>\n",
       "      <td>0.099698</td>\n",
       "      <td>0.314348</td>\n",
       "      <td>0.671199</td>\n",
       "      <td>0.092040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td></td>\n",
       "      <td>2019-03-18 20:03:19</td>\n",
       "      <td>2.517 sec</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.324000</td>\n",
       "      <td>1.342693</td>\n",
       "      <td>0.099118</td>\n",
       "      <td>0.313877</td>\n",
       "      <td>0.670180</td>\n",
       "      <td>0.092869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td></td>\n",
       "      <td>2019-03-18 20:03:19</td>\n",
       "      <td>2.651 sec</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.324013</td>\n",
       "      <td>1.317427</td>\n",
       "      <td>0.100162</td>\n",
       "      <td>0.313953</td>\n",
       "      <td>0.670830</td>\n",
       "      <td>0.092869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td></td>\n",
       "      <td>2019-03-18 20:03:19</td>\n",
       "      <td>2.805 sec</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0.324427</td>\n",
       "      <td>1.297244</td>\n",
       "      <td>0.099466</td>\n",
       "      <td>0.314128</td>\n",
       "      <td>0.671509</td>\n",
       "      <td>0.093698</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                timestamp    duration  number_of_trees  training_rmse  \\\n",
       "0     2019-03-18 20:03:16   0.023 sec              0.0            NaN   \n",
       "1     2019-03-18 20:03:16   0.117 sec              1.0       0.612213   \n",
       "2     2019-03-18 20:03:16   0.154 sec              2.0       0.506486   \n",
       "3     2019-03-18 20:03:16   0.191 sec              3.0       0.441045   \n",
       "4     2019-03-18 20:03:16   0.245 sec              4.0       0.421699   \n",
       "5     2019-03-18 20:03:16   0.318 sec              5.0       0.418647   \n",
       "6     2019-03-18 20:03:17   0.376 sec              6.0       0.397057   \n",
       "7     2019-03-18 20:03:17   0.427 sec              7.0       0.377551   \n",
       "8     2019-03-18 20:03:17   0.484 sec              8.0       0.369709   \n",
       "9     2019-03-18 20:03:17   0.540 sec              9.0       0.356990   \n",
       "10    2019-03-18 20:03:17   0.600 sec             10.0       0.345275   \n",
       "11    2019-03-18 20:03:17   0.663 sec             11.0       0.335225   \n",
       "12    2019-03-18 20:03:17   0.734 sec             12.0       0.332255   \n",
       "13    2019-03-18 20:03:17   0.808 sec             13.0       0.331825   \n",
       "14    2019-03-18 20:03:17   0.882 sec             14.0       0.328639   \n",
       "15    2019-03-18 20:03:17   0.956 sec             15.0       0.325734   \n",
       "16    2019-03-18 20:03:17   1.043 sec             16.0       0.325565   \n",
       "17    2019-03-18 20:03:17   1.127 sec             17.0       0.323582   \n",
       "18    2019-03-18 20:03:17   1.226 sec             18.0       0.325102   \n",
       "19    2019-03-18 20:03:17   1.328 sec             19.0       0.328760   \n",
       "20    2019-03-18 20:03:18   1.423 sec             20.0       0.327908   \n",
       "21    2019-03-18 20:03:18   1.522 sec             21.0       0.326994   \n",
       "22    2019-03-18 20:03:18   1.624 sec             22.0       0.326306   \n",
       "23    2019-03-18 20:03:18   1.727 sec             23.0       0.320351   \n",
       "24    2019-03-18 20:03:18   1.846 sec             24.0       0.322154   \n",
       "25    2019-03-18 20:03:18   1.963 sec             25.0       0.322747   \n",
       "26    2019-03-18 20:03:18   2.085 sec             26.0       0.324526   \n",
       "27    2019-03-18 20:03:18   2.208 sec             27.0       0.324279   \n",
       "28    2019-03-18 20:03:19   2.385 sec             28.0       0.325019   \n",
       "29    2019-03-18 20:03:19   2.517 sec             29.0       0.324000   \n",
       "30    2019-03-18 20:03:19   2.651 sec             30.0       0.324013   \n",
       "31    2019-03-18 20:03:19   2.805 sec             31.0       0.324427   \n",
       "\n",
       "    training_logloss  training_classification_error  validation_rmse  \\\n",
       "0                NaN                            NaN              NaN   \n",
       "1          12.228297                       0.355035         0.626379   \n",
       "2           8.307597                       0.250291         0.405501   \n",
       "3           6.074987                       0.193444         0.339030   \n",
       "4           5.335349                       0.178799         0.336118   \n",
       "5           5.102575                       0.177760         0.337905   \n",
       "6           4.372776                       0.158025         0.329788   \n",
       "7           3.718919                       0.143737         0.320569   \n",
       "8           3.451601                       0.137274         0.322780   \n",
       "9           3.065563                       0.127815         0.318121   \n",
       "10          2.742843                       0.118047         0.311870   \n",
       "11          2.446840                       0.111345         0.307735   \n",
       "12          2.334162                       0.109248         0.307513   \n",
       "13          2.210141                       0.108708         0.306161   \n",
       "14          2.110953                       0.106657         0.306540   \n",
       "15          1.995089                       0.103949         0.304762   \n",
       "16          1.908639                       0.103564         0.305527   \n",
       "17          1.803415                       0.102159         0.305310   \n",
       "18          1.766299                       0.103076         0.308479   \n",
       "19          1.753563                       0.104457         0.312366   \n",
       "20          1.690811                       0.104457         0.312430   \n",
       "21          1.631042                       0.103644         0.312311   \n",
       "22          1.586581                       0.102948         0.311774   \n",
       "23          1.490570                       0.098654         0.307810   \n",
       "24          1.463251                       0.099234         0.310267   \n",
       "25          1.428161                       0.099466         0.311251   \n",
       "26          1.415446                       0.099234         0.313407   \n",
       "27          1.407645                       0.099582         0.313540   \n",
       "28          1.373881                       0.099698         0.314348   \n",
       "29          1.342693                       0.099118         0.313877   \n",
       "30          1.317427                       0.100162         0.313953   \n",
       "31          1.297244                       0.099466         0.314128   \n",
       "\n",
       "    validation_logloss  validation_classification_error  \n",
       "0                  NaN                              NaN  \n",
       "1            12.680359                         0.397181  \n",
       "2             4.678126                         0.170813  \n",
       "3             2.649073                         0.120232  \n",
       "4             2.256579                         0.116086  \n",
       "5             2.027977                         0.117745  \n",
       "6             1.706817                         0.109453  \n",
       "7             1.452873                         0.102819  \n",
       "8             1.429145                         0.101990  \n",
       "9             1.318139                         0.097844  \n",
       "10            1.202616                         0.090381  \n",
       "11            1.091966                         0.093698  \n",
       "12            1.063560                         0.092869  \n",
       "13            0.979710                         0.086235  \n",
       "14            0.980853                         0.087894  \n",
       "15            0.950863                         0.088723  \n",
       "16            0.851402                         0.088723  \n",
       "17            0.851793                         0.092040  \n",
       "18            0.807913                         0.094527  \n",
       "19            0.817136                         0.093698  \n",
       "20            0.768916                         0.093698  \n",
       "21            0.769718                         0.092869  \n",
       "22            0.768383                         0.092869  \n",
       "23            0.734756                         0.092040  \n",
       "24            0.740826                         0.092869  \n",
       "25            0.715792                         0.092869  \n",
       "26            0.721587                         0.092869  \n",
       "27            0.695037                         0.092040  \n",
       "28            0.671199                         0.092040  \n",
       "29            0.670180                         0.092869  \n",
       "30            0.670830                         0.092869  \n",
       "31            0.671509                         0.093698  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# deep learning parameters\n",
    "rf_model_h = H2ORandomForestEstimator(ntrees=200, stopping_rounds=2, seed=1234)\n",
    "rf_model_h.train(x=x, y=y, training_frame=train_h, validation_frame=valid_h)\n",
    "# show accuracy\n",
    "rf_model_h._model_json['output']['scoring_history'].as_data_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is divergence in the results of the means shift method between the holdout and training group. Since the holdout group is a random sampling of the whole dataframe, only certain words from the BOW model will carry through to the holdout group. So even if all authors are in the holdout, not all the words will be which is likely the reason for the fewer amount of clusters. \n",
    "\n",
    "The same logic pertains to the affinity propogation algorithm. While the affinity propogation model overestimates the number of clusters, something interesting happens when you look at the difference in the number of clusters in the two methods: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The means shift holdout clusters are 28.947368421052634% of the training frame\n",
      "The affinity propogation holdout clusters are 33.33333333333333% of the training frame\n"
     ]
    }
   ],
   "source": [
    "# show percentage of clusters in holdout compared to train for each model\n",
    "print('The means shift holdout clusters are {}% of the training frame'.format((22/76)*100))\n",
    "print('The affinity propogation holdout clusters are {}% of the training frame'.format((2546/7638)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The similarity in cluster groupings between each method show that the algorithms are finding simliar cluster structure between the frames. The fact that the groupings follow a similar pattern show that authors are consistently grouped into the same clusters. Means shift is the better option on this data as it does not overestimate the number of clusters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZYAAAEeCAYAAACpGzMjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzs3Xl81NW5+PHPk41sJIGEPYSw74IQdlyoomhVWpdbl7or19atWq9yb/21VmvrrUtFpVZUFpdqafW2tMWlWm1lEQjIGhTCHhZDwr4mkzy/P843wxhCMsBkJpk879eLVzLf+X5nni8DeXLOec45oqoYY4wxoRIT6QCMMcZEF0ssxhhjQsoSizHGmJCyxGKMMSakLLEYY4wJKUssxhhjQsoSizHGmJCyxGKijoicKyJa7c8BEVkiIveJSFwN13zqnVcuIm1P8LqTAl7v3GrPdRGRKSLypYgcEpHdIlIgIjNEZEy1czfWEF/gn+8HcY8bRWTlSf7VGBMWx/0HMyaKvAXMBgRoC9wAPAP0BibUcL7P+3o98GTgEyKSAFwHHAESqz2XB/wLKAdeA1YBSUAP4FJgP/BJtfcqAv77BHHPrfPOjGnALLGYaLZEVd+oeiAivwW+BG4TkZ+o6s5q5x8F/gncTLXEAowHMoHfA9dWe+5nQDJwpqouDXxCRO7CJbXq9gbGZkw0sa4w02So6kHgc1wLpusJTpsG9BaRYdWO3wwsA76o4ZruQGn1pOK9Z6Wqbjv1qE+fiNzmdQMeFpG9IvKhiIyu4bxvi8i/RKTEO3eziLwrIj0CzukoIlNFZJOIHBWRYhGZJyI3hveuTENmicU0NVUJZdcJnv8bUAzcUnVARNoDFwBTT3DNOiBTRC4/iThiRSTrBH/kJF6nViLyv8DLuG66/wGeBvoAn4jIxQHnnQPMAtKBXwF3eddlAt28c+KAfwBXAW8DPwSeANYAZ4UqZtP4WVeYiWbJIpLFsTGWO4AzgUWquqamC1S1XETewHWX/UhVDwM3ARXAm7iWS3W/AMYC74jIWmAOsAj4VFVXnyC2XkD1rrgqrYCSIO6vViLSE/gv3JjNt1S1zDv+ClAA/FZEuqpqBa6rLwa4QFWLA17msYDv+wA9gYdU9denG5+JXtZiMdHs57gf3sXActxv2O8Cl9Vx3VQgDahqgdwE/EVVS2s6WVXnA4OBGbjf+G8GfgsUiMhnItKlhss24pJRTX/2BnV3dRuPS6q/rkoqXrzbgOlAJ1yiJeA9r6ipaq7aOWNEpHWIYjRRyFosJppNAf4IxAP9gYeAbFxl1wmp6ioRWQTcLCKbcWMo99ZxzQpcAkJEOgHnALfhuoj+IiKDA3+4AwdV9aNTuamT0Nn7uqqG56pKlbsA+cALuET0W+B/RWQO8D7wVlWRg6puEpHHcdVs20VkKfAx8EdVXVR/t2EaG2uxmGi2VlU/UtX3vK6bS4EhwO+CuHYq8C1cxddW4MNg31RVN6nqa7jkMhfoBww92eBDIOixGq81NgQYAzwPNAd+A6wRkREB5z2MS7Q/wo0t3QYs9MZyjAEssZgmRFXnAa8D3xORkXWc/hauZXMeMMMbhzjZ91Nggfeww8leHwLrvK99a3iuj/d1fdUBVa1Q1U9V9SeqehaumywVeDjwQlVdr6rPq+p/AO2BfwMPWveYqWKJxTQ1j+EG4h+t7SRV3Ysb7P858FJt54rI2BPM5k/CVZOBGywPt1mAAv8lIvEBcbXDjQNtwiuf9oocqvsSOAy09M5JD3wdAFU9AlQVKLQI9Q2YxsnGWEyToqqFIvI2cJ2InKWqn9Vy7mtBvuxvcOXGs4AVwCGgI24iZQ/gNW8MJlB6LUu3rFDVZUG8bysRefgEz01T1a9E5EngQeDfIvIHXBfXBFxL5LqAltjLIpKN6/LbhFs54Hve+VV/D2OAKSLyDvAVcABXtHAbsEBVvwoiZtMEWGIxTdHjwDW4VsuYOs4Nxv24ge/RwBVABq6Cajnwv7gKrOqycd1yJ4ovmMTSmm+WAwf6CNiqqg+JSCHH5pyU4brnrq2WVF/HFR/ciCt33odrZV2pqu945yzDVdWdi1veJhbYDPwSNz/GGADEdQMbY4wxoWFjLMYYY0LKEosxxpiQssRijDEmpCyxGGOMCakmWRWWlZWlubm5kQ7DGGMalcWLF5eoaqu6zmuSiSU3N5f8/PxIh2GMMY2KiGwK5jzrCjPGGBNSlliMMcaElCUWY4wxIdUkx1hqUl5eTlFREUeO1LpVh/EkJiaSnZ1NfHx83ScbY5oUSyyeoqIimjdvTm5uLiHccjwqqSqlpaUUFRXRuXPnui8wxjQp1hXmOXLkCJmZmZZUgiAiZGZmWuvOGFMjSywBLKkEz/6ujDEnYonFGGOiwcES+Oxp2LIo0pFYYmkoSktLGThwIAMHDqRt27Z06NDB/7isrCyo17j55pv56qva91qaPHkyb775ZihCNsZEmqpLJO9OgGd6w8ePwvpPIh2VDd43FJmZmSxduhSARx55hNTUVB544IFvnKOqqCoxMTX/PjBt2rQ63+fOO+88/WCNMZFVdghWvgOLXobtyyChOQy+CYbcBq16Rjo6a7E0dIWFhfTr14877riDQYMGsX37diZMmEBeXh59+/bl0UePbd0+evRoli5dis/nIyMjg4kTJzJgwABGjBhBcXExAA8//DDPPvus//yJEycydOhQevbsybx58wA4ePAgV1xxBQMGDOCaa64hLy/Pn/SMMRG0az188BPXOpl1F/jK4NtPw49Xw8VPNoikAtZiqdHP/7qKgm37Qvqafdqn8bNL+57StQUFBUybNo3f/e53ADzxxBO0bNkSn8/HmDFjuPLKK+nTp883rtm7dy/nnHMOTzzxBPfffz9Tp05l4sSJx722qrJw4UJmzZrFo48+yvvvv8/zzz9P27Zteeedd1i2bBmDBg06pbiNMSFQWQGFH8HCl93XmFjodQkMvR06jYIGWEhjiaUR6Nq1K0OGDPE/fuutt3j11Vfx+Xxs27aNgoKC4xJLUlISF110EQCDBw/ms88+oyaXX365/5yNGzcCMGfOHB566CEABgwYQN++p5YQjTGn4dAu+OJ1WPQq7NkEqW3hnIdcl1dau0hHVytLLDU41ZZFfUlJSfF/v3btWiZNmsTChQvJyMjg+9//fo3zSRISEvzfx8bG4vP5anztZs2aHXeOqoYyfGPMydi6BBa94sZQfEdcq+T8R6D3pRDbOFa6sMTSyOzbt4/mzZuTlpbG9u3b+eCDDxg3blxI32P06NHMnDmTs846ixUrVlBQUBDS1zfGVFN+BFb9nxuM37oY4lNg4LVuML5Nw/pFNxiWWBqZQYMG0adPH/r160eXLl0YNWpUyN/j7rvv5oYbbuCMM85g0KBB9OvXj/T09JC/jzFN3u5NkD/VdXkdKoWsHnDRr2HA1ZDYeP/PSVPs9sjLy9PqG32tXr2a3r17RyiihsXn8+Hz+UhMTGTt2rVccMEFrF27lri4b/4eYn9nxpyCykpY/09Y+Aqsed8Nvve82A3Gdz6nQQ7GVxGRxaqaV9d51mIxxzlw4ADnnXcePp8PVeWll146LqkYY07S4d2w9Pdu/GTXekhpBWf9GPJuhvTsSEcXUmH9aSEi44BJQCzwiqo+Ue35HGAGkOGdM1FVZ4tIAvASkAdUAveq6qfeNQnAC8C53nM/UdV3wnJDUSojI4PFixdHOgxjosP25W7sZPkfwXcYOg6Dc/8H+lwGcc0iHV29CFtiEZFYYDIwFigCFonILFUNHBl+GJipqi+KSB9gNpAL3A6gqv1FpDXwnogMUdVK4CdAsar2EJEYoGW47skYY2rkK4OCv7iEsmUBxCXBGVfBkNuh3RmRjq7ehbPFMhQoVNX1ACLyNjAeCEwsCqR536cD27zv+wAfA6hqsYjswbVeFgK3AL285yqBkvq9DWOMOYG9RZA/DZbMgIM7oWUXuPCXrsIrqUWkowubcCaWDsCWgMdFwLBq5zwCfCgidwMpwPne8WXAeC8ZdQQGAx1FZI33/GMici6wDrhLVb+u/uYiMgGYAJCTkxOK+zHGGLcQ5IZ/uZnxX812j3uMg6G3QZdvwQnW9otm4UwsNZU6VC9JuwaYrqpPi8gI4HUR6QdMBXoD+cAmYB7gw8WfDcxV1ftF5H7gKeD6495IdQowBVxVWGhuyRjTZB3ZC8vedoPxJWsgqSWMvAfyboEWnSIdXUSFM5UW4VobVbI51tVV5VZgJoCqzgcSgSxV9anqfao6UFXH4wb31wKlwCHg/7zr/wg02oWtUlNTv/F4+vTp3HXXXbVe88gjj/DUU08dd3zjxo3069fvlGP55S9/ecrXGhPVvi6Av90HT/eG9x6EZs3hO7+D+1fD2J83+aQC4U0si4DuItLZq+S6GphV7ZzNwHkAItIbl1h2ikiyiKR4x8cCPlUtUDcJ56+4ijC8a22aeAhYYjEmQEU5rHwXpl0ML46AL96Evt+B2z+B2/8JA6+B+MRIR9lghK0rTFV9InIX8AGulHiqqq4SkUeBfFWdBfwYeFlE7sN1k92kqupVgn0gIpXAVr7Z1fUQrsvsWWAncHO47imcNm3axC233MLOnTtp1aoV06ZNO26saPHixdxyyy0kJyczevRo//EjR47wgx/8gPz8fOLi4njmmWcYM2YM06dPJz8/nxdeeAGASy65hAceeID333+fw4cPM3DgQPr27Wsbg5mma992WDzd/TmwAzI6wdhH4czrIdkKUE8krPNYVHU2roQ48NhPA74vAI5bo0RVNwI1bjSgqpuAs0Ma6HsTYceKkL4kbfvDRU/UekrVD/Mqu3bt4rLLLgPgrrvu4oYbbuDGG29k6tSp3HPPPfz5z3/+xvU333wzzz//POeccw7/9V//5T8+efJkAFasWMGXX37JBRdcwJo1aziRJ554ghdeeMH2YDFNkypsmusG47/8m1u2vtv5MPQ59zUmNtIRNng2nboBSUpK+sYP86oWBcD8+fN59913Abj++ut58MEHv3Ht3r172bNnD+ecc47/nPfeew9wy+DffffdAPTq1YtOnTrVmliMaZKO7oflf3BLrexcDYkZMOwOGHKrKxs2QbPEUpM6WhYNgVRbT0hVjzsW+FxN4uLiqKys9D+uafl9Y6Lezq9cZdfSt6BsP7QbAJe9AP2ugITkSEfXKDW9AutGauTIkbz99tsAvPnmm98YQwG3DEt6ejpz5szxn1Pl7LPP9j9es2YNmzdvpmfPnuTm5rJ06VIqKyvZsmULCxcu9F8THx9PeXl5fd+WMZFR4YOCWTDjUpg81I2h9LoYbv0IJvwLBl1vSeU0WIulkXjuuee45ZZbePLJJ/2D99VNmzbNP3h/4YUX+o//8Ic/5I477qB///7ExcUxffp0mjVrxqhRo+jcuTP9+/enX79+39iCeMKECf5l823w3kSNA8WweAYsngb7tkJ6Rzjvp3DmDZDaKtLRRQ1bNt9jS8CfPPs7M42Cqluva+HLbv2uynLoMsYtU9/9Qoi136+DZcvmG2OatrKDsOKPbjD+6xXQLN3tyDjkVsjqHunoopolFmNMdCld5wbjv3gTju6FNv3gkmfhjP+AhJRIR9ckWGIJUFtllfmmptiFahqwygpY84Fbpn7dPyEmDvqMd8vU5wxv0LsyRiNLLJ7ExERKS0vJzMy05FIHVaW0tJTERFvCwkRY2SFY+ibMex72bILm7WHMT2DQjdC8TaSja7IssXiys7MpKipi586dkQ6lUUhMTCQ7O7q2UzWNyMFS1zpZ8BIc3gXZQ9xSK72+DbHxkY6uybPE4omPj6dz586RDsMYU5vdG2H+ZFjyutvmt8dFMOpe6+5qYCyxGGMavu3LYO4kWPV/ILFwxvdg5N3QulekIzM1sMRijGmYVGH9Jy6hrP8UEprDiLtg+A8grX2kozO1sMRijGlYKnxQ8GeY+6xbZTy1DZz/iNuZMTE90tGZIFhiMcY0DGUH3dyT+c/Dns2Q2R0ue951e8U1i3R05iRYYjHGRNbBErfcysIprsKr4zAY94QbmI+xdXIbI0ssxpjI2LXBVXh98Yar8Op58bEKL9OoWWIxxoTXti9g7nNuHEViYcD3YOQ90KrGTWJNI2SJxRhT/1TdUitzJ8GGf0GzNFcuPOwHkNYu0tGZEAtrYhGRccAkIBZ4RVWfqPZ8DjADyPDOmaiqs0UkAXgJyAMqgXtV9dNq184Cuqhqv3q/EWNMcCp8bu7J3EluheHUtm6G/OCbrMIrioUtsYhILDAZGAsUAYtEZJaqFgSc9jAwU1VfFJE+wGwgF7gdQFX7i0hr4D0RGaKqld5rXw4cCNe9GGPqUHbQzY6fPxn2boasHjB+MvS/yiq8moBwtliGAoWquh5ARN4GxgOBiUWBNO/7dGCb930f4GMAVS0WkT241stCEUkF7gcmADPr+yaMMbU4sNNVdy16GQ7vho7D4eJfuw21rMKryQhnYukAbAl4XAQMq3bOI8CHInI3kAKc7x1fBoz3klFHYLD3dSHwGPA0cKi2NxeRCbjkQ05OzunchzGmul3rYd4LbqVh3xHodYkbkM+p/l/cNAXhTCw1rRBXfVOPa4Dpqvq0iIwAXheRfsBUoDeQD2wC5gE+ERkIdFPV+0Qkt7Y3V9UpwBRwWxOfzo0YYzxbl8C859yWvzFxMOBqGHE3tOoR6chMBIUzsRThWhlVsjnW1VXlVmAcgKrOF5FEIEtVi4H7qk4SkXnAWuAcYLCIbMTdS2sR+VRVz62vmzCmyVOFdR97FV7/9iq87oFhd1iFlwHCm1gWAd1FpDOwFbgauLbaOZuB84DpItIbSAR2ikgyIKp6UETGAj5v0L8AeBHAa7H8zZKKMfWkojygwmslNG8HYx/zKrzS6rzcNB1hSyyq6hORu4APcKXEU1V1lYg8CuSr6izgx8DLInIfrpvsJlVVrxLsAxGpxCWl68MVtzFN3tED8EVVhdcWaNULxv/Wq/BKiHR0pgGSprh3eV5enubn50c6DGMatgM7YeFLbh2vI3sgZ6RbcqX7BVbh1USJyGJVzavrPJt5b4z5ptJ1MP8FWPp78B112/2Ouhc6Do10ZKaRsMRijHG2LnbjJwWz3L7xA65xy65kdY90ZKaRscRiTFOmCoUfu021Nn4GzdJh9H0w7D+hedtIR2caKUssxjRFFeWw8l3XQileBc3bwwWPw+AboVnzSEdnGjlLLMY0JUcPwJLXXIXXviJo1Ru+8yL0u9IqvEzIWGIxpik4UAwLXoJFr7gKr06j4JJnoNtYq/AyIWeJxZhoVroO5j3vKrwqyqD3JTDyXug4JNKRmShmicWYaFS02A3Ir/4rxCbAwGvcGl5Z3SIdmWkCLLEYEy1UYe0/3ID8pjluI62z7oeh/wnN20Q6OtOEWGIxprHzlcHKd9wqw8UFkNYBLvwlDLrBKrxMRFhiMaaxOrofFs+Az38L+7ZC6z7w3Zeg3xVugqMxEWKJxZjGZv/Xbg2vRa/Akb2QexZcOgm6nQ9S07ZHxoSXJRZjGouSQpj/PCx9y6vwutSt4ZVd55qAxoSVJRZjGrqifJjzG/jy716F17VuDa/MrpGOzJga1ZlYRCQOt1f8n1W1+o6Pxpj6UFkJhVUVXnO9Cq8fuzW8UltHOjpjalVnYvE26HoS+HsY4jGmafOVwco/wdznYOdqSMuGC38Fg663Ci/TaATbFfY5MAjYVI+xGNN0HdkHS2bA/N/C/m3Qui98dwr0u9wqvEyjE2xieRl4WkQ6AYuBg4FPquqSUAdmTJOwfwcs+B0smgpHvQqvy56HbudZhZdptIJNLL/3vj5Tw3OK28O+TiIyDpjknf+Kqj5R7fkcYAaQ4Z0zUVVni0gC8BKQB1QC96rqpyKSDPwR6ApUAH9V1YlB3pMxkVOy1k1oXPY2VPqg92Uw6h7oMDjSkRlz2oJNLJ1P941EJBaYDIwFioBFIjJLVQsCTnsYmKmqL4pIH2A2kAvcDqCq/UWkNfCeiFStoveUqn7iJZ+PReQiVX3vdOM1pl5sWegG5L/8O8Q1gzOvhxF3WoWXiSpBJRZVDcXYylCgUFXXA4jI28B4IDCxKJDmfZ8OVFWh9QE+9mIpFpE9QJ6qLgQ+8Y6XicgSIDsEsRoTOpWVsPYDl1A2z4fEDDj7AbeGV2qrSEdnTMgFPY9FRM4AHsD9kFdcQnhKVVcE+RIdgC0Bj4uAYdXOeQT4UETuBlKA873jy4DxXjLqCAz2vi4MiC8DuBTX1VZT/BNwZdPk5OQEGbIxp8FXBiv+6Lq8dn4J6R1h3BOuldIsNdLRGVNvgkosInIZ8C7wGVDVzTQaWCIil6vqX4N5mRqOabXH1wDTVfVpERkBvC4i/YCpQG8gH1eZNg/wBcQXB7wFPFfVIjrujVSnAFMA8vLyqr+vMaFzZB8snu7W8Nq/Hdr0g8tfhr7ftQov0yQE22L5BfC4qv4s8KCIPOo9F0xiKcK1Mqpkc6yrq8qtwDgAVZ0vIolAlqoWA/cFvO88YG3AdVOAtar6bHC3Y0w92L8DPn8R8qfC0X3Q+WwY/wJ0tQov07QEm1h6AK/XcPx14MEgX2MR0F1EOgNbgauBa6udsxk4D5guIr2BRGCnV/0lqnpQRMYCvqpBfxH5BW485rYg4zAmtHaucd1dy//gKrz6jIeR90CHQZGOzJiICDaxFOPGNQqrHR8MfB3MC3gz+O8CPsCVEk9V1VVeqydfVWcBPwZeFpH7cN1kN6mqepVgH4hIJS4pXQ8gItnAT4Avcd1yAC+o6itB3pcxp27zAjcg/9XfIS7R7X8y4k5o2SXSkRkTUSczQfIlEemGG99Q3BjLA8CTwb6Zqs7GlRAHHvtpwPcFwKgartsI9KzheBE1j90YUz8qK2HN+y6hbPkcklrAOQ/B0AmQkhXp6IxpEE5mjOUArkXxmHdsG/Az4Ll6iMuYhsV3FJbPdF1eJWsgPQcu+jWc+X1ISIl0dMY0KMGsbhwD9AKmqOpvRKQ5gKrur+/gjIm4I3shf5oblD+wA9r0hytehT7fgVjbdcKYmgTzP0OBpbj5K4WWUEyTsG+7KxfOnwZl+6HzOfCd30LXb1mFlzF1CGbZfBWRr4BWHD94b0x02fmVt4bXH0ArXMtk1D3Q/sxIR2ZMoxFsW/5B4EmvqmuZqtoEQxNdNn/uVXjNhrgkGHyTV+F12svkGdPkBJtYZuLmlCwGfCJyNPBJVU2r8SpjGrLKSljznlfhtQCSWsI5E2Ho7VbhZcxpCDax3FWvURgTTr6jbjLj3OegdC1k5MBFT8KZ11mFlzEhEOye9ynYnvemsTu8BxZXVXh9DW3PsAovY+qB7Xlvot++bV6F13RX4dVlDHz3JehyrlV4GVMPTmbP+8HYnvemMSn+0lvDa6ar8Or7XRh1L7QbEOnIjIlqJ7Oky1Pe1sG2571puFTdZlpzJ7mlV+KSIO8WGPFDaJEb6eiMaRLCuue9MfWmstKVCs+dBEULITkTzv0fGHIbpGRGOjpjmpSw7XlvTL0oP+IqvOY9B6WFkNEJLn4KBl4HCcmRjs6YJimce94bEzqH97gNtRb8zlV4tRsAV06F3uOtwsuYCDuZPe8vAu4EugAXquoWEbkN2KCqH9dXgMZ8w96trsJr8XQoO+DW7rp8ilvLyyq8jGkQgt3z/jrgd8AruB0eqzbujsUt92KJxdSvrwtg3vOwYqYboO93udulsd0ZkY7MGFPNyawVdruqvu21Uqp8Djwa+rCMwSWQTfPcgPzaDyA+2Q3GD/8htOgU6eiMMScQbGLpDsyv4fgBwNYJM6FVWQFf/t0llK35rsJrzE9cUkluGenojDF1iAnyvG1AjxqOnw2sC/bNRGSciHwlIoUiMrGG53NE5BMR+UJElovIxd7xBBGZJiIrRGSZiJwbcM1g73ihiDwnYh3tjVb5Ebf/yQtDYOb1cKgEvv00/GglnPOgJRVjGolgWyxTgOcCusE6ishZwK+BR4J5ARGJBSYDY4EiYJGIzPL2ua/yMDBTVV8UkT7AbCAXuB1AVfuLSGvgPREZoqqVwIvABFy33GxgHPBekPdlGoLDu12F1+e/g4PF0G4gXDUdel8GMTZFypjGJthy41+LSDrwD9zy+Z8AR4GnVHVykO81FLcD5XoAEXkbGA8EJhblWNdaOq6lBG73yo+9WIpFZA+QJyJbgDRVne+95mvAd7DE0jjsLXILQvorvM5zS650PtsqvIxpxIIuN1bVn4jI47gf8jFAgaoeOIn36gBsCXhcBAyrds4jwIcicjduReXzvePLgPFeMuqIW7esI1DpvU7ga3ao6c1FZAKuZUNOTs5JhG1C7utVbsn6lX/yKryucLs0tu0f6ciMMSFwUjPJVPUQkH+K71XTr6DVd6K8Bpiuqk+LyAjgdRHpB0wFenvvvQmYB/iCfM2q2KfguvTIy8uzHTDDTRU2zfUqvD70Krxud2t4ZViiNyaahHOKchGulVElm2NdXVVuxY2RoKrzRSQRyFLVYuC+qpNEZB6wFtjtvU5tr2kiqbICvvybV+G1GJKzYMzDMORWG4w3JkqFM7EsArqLSGdgK3A1cG21czbjJmBOF5HeuPGcnSKSDIiqHhSRsYCvatBfRPaLyHBgAXAD8Hx4bsfUqvwwLHvLTWrctR5adIZvPwMDr4X4pEhHZ4ypR2FLLN6GYXcBH+Bm7E9V1VUi8iiQr6qzgB8DL4vIfbgurZtUVb1KsA9EpBKXlK4PeOkfANOBJNygvQ3cR9KhXZD/Kix4CQ7uhPZnwlUzoPelVuFlTBMhqk1vuCEvL0/z8091qMjUaM8Wbw2vGVB+ELqNdRVeuaOtwsuYKCEii1U1r67zam2xiMgvgV94g/Z4ExY/UdXD3uM04AVVvSEEMZvGaMdKt2T9ynfc435Xwsi7oW2/yMZljImYurrCHgKeBQ55j98GBgLrvcdJwHW4sQ3TVKjCxs/cgHzhRxCfAkMnuDW8MjrWfb0xJqrVlViq92FYn0ZTVlkBq//qEsq2JZDSCr71MORZhZcx5hjbEcnUrfwwLP29q/DavQFadoFLfgMDrrEKL2PMcSyxmBM7tAsWveoTNv8NAAAf3klEQVR2aTxUAu0HwdifQ69LrMLLGHNCwSSWO0SkaumWOOBWESn1Hjevn7BMRO3ZDPN/C0tecxVe3S9wFV6dRlmFlzGmTnUlls3AzQGPd1DzpEYTDXas8NbwesclkP5XuQqvNn0jHZkxphGpNbGoam6Y4jCRogob/u0G5Nd9DAmpMPwH7k96dt3XG2NMNTbG0lRV+GD1LJdQti+FlNZw3k8h7xZIahHp6IwxjVhdEyQHAC1V9ZOAY9cBjwGpwLvAPapaVq9RmtApOwRL34T5L8DujdCyK1w6Cc64GuITIx2dMSYK1NVi+QWwELexF96ujtO8x18Ct+DW7nqsHmM0oXBoFyx8GRa+BIdKocNgGPsY9Pq2VXgZY0KqrsQyCHg84PHVuA2+LgQQkeW45ewtsTRUuze5NbyWvAblh6D7hV6F10ir8DLG1Iu6EksmrkVS5WzgrwGPPwV+E+KYTChsX+6t4fWuV+H1H16FV59IR2aMiXJ1JZadeFsKi0gsbkvgpwKeT8BtD2waAlXY8C+vwuufARVeP4T0GndsNsaYkKsrsXwK/ExE7gSu9I59EvB8H2Bj6MMyJ6XCB6v/4lV4LfMqvH7mVXhlRDo6Y0wTU1di+X/AR0AhUIGrADsY8Pz1wMf1FJupS1WF17znYc8myOwGlz4HZ3zPKryMMRFT1wTJjSLSC+gL7FTV6vvJ/wy3l70Jp4OlsOhlt0vj4V2QPQQu/CX0vBhiYiIdnTGmiatzgqSq+oBlJ3iuxuOmnuzeCPMnw5LXwXcYeoyDUT+CnOFW4WWMaTDqmiB5fzAvoqrPhCYcU6Pty9z4yar/A4mFM7wKr9a9Ix2ZMcYcp64Wy1NACXCAE2/ypUBQiUVExgGTgFjgFVV9otrzOcAMIMM7Z6KqzhaReOAV3LyaOOA1Vf2Vd819wG1eHCuAm1X1SDDxRML7K3cwoGM67dLr2MdEFdZ/6hLK+k8goTmMuBOG/cAqvIwxDVpdiSUfV/n1d+BVVZ1zqm/klStPBsbixmUWicgsVS0IOO1hYKaqvujN8p8N5AJXAc1Utb+IJAMFIvIWUA7cA/RR1cMiMhM3iXP6qcZZn0oPHOWONxYD0KVVCqO7ZTGqWxYjumaSlhjvTqrwQcGfXULZsRxS28D5j7gKr8T0iMVujDHBqmvwfqiI9AVuBd4Vkd3Aq8AMVf36JN9rKFCoqusBRORtYDwQmFgUSPO+Twe2BRxPEZE4IAkoA/Z538cBSSJSDiQHXNPgtExJ4L17z2JuYQlzCkv4Y34Rr83fRIzA0OxEbk+dx6idb5N4oAgyu8Nlz7sKr7hmkQ7dGGOCJqoa3ImuO2o8bn2wMcCHwH+o6tEgr78SGKeqt3mPrweGqepdAee08163BZACnK+qi733fh04D5c87lPVKd419+KWnTkMfKiq153g/ScAEwBycnIGb9q0Kaj7rk9lvkpWrCmkbN5L9N06kzTdx+LK7kzV8RzMHcuo7q0Z1S2LXm2bExNjg/PGmMgSkcWqmlfXeUEvm6+q5cCfRGQf7of7t3EthqASCzWP0VTPatcA01X1aREZAbwuIv1wrZ0KoD0u6XwmIh8Bu3HJrjOwB/ijiHxfVd+oIf4pwBSAvLy84LJpfdq1gYT5kxn8xRuuwqvnxRwccie7yrrTqrCErwpLeHz2agAyUxIY2S2L0d0yGdUti+wWyREO3hhjTiyoxCIiubiWyo3eoddwg+R7TuK9ioCOAY+zOb7b6lZgHICqzheRRCALt2vl+15yKxaRuUAeLjFtUNWdXpzvAiOB4xJLg7HtC7dLY8GfXYXXgO/ByHugVU9ScANQY/u0AWDH3iPMLSzxd539dZn768rNTGZUtyxGe+MzGckJkbsfY4yppq5y42txP+xH4Baf/E/gAw22/+ybFgHdRaQzbmHLq6l5m+PzgOki0htIxK1Xthn4loi8gWstDQeexbWYhnsD+oe9a/NPIbb6perW7po7ya3l1SzNlQsPuwPS2p/wsrbpiVwxOJsrBmejqhQWH+CztS7R/PmLrby5YLNbX7JDOqO6ZXFWtywGdWpBYrwtg2+MiZxax1hEpBL3Q/33uLLjGgU7j0VELsYlhFhgqqo+LiKPAvmqOsurBHsZt4mYAg+q6ocikorbB6YPrkttmqo+6b3mz4HvAT7gC+C2usZ98vLyND8/DPmnwufmnsydBF+vgNS2MOKHMPim067wKq+oZNmWPczxWjRfbN6Dr1JpFhfD0M4t/S2aPu3SbHzGGBMSwY6x1JVYNnL8OEh1qqpdTi68yKr3xFJ2EL54A+a9AHs3Q1YPtwdK/6vqrcLrwFEfCzeUMmdtKXMLS/jq6/0AtEiOZ2TXLH+iycm08RljzKkJSWKJVvWWWA6WwMIp7s/h3dBxOIz+kdtcK8xreBXvO8K8daXMKSxhztoSduxzc0Y7tkzyz58Z2TWLlik2PmOMCU7YEouIdFTVLaf1ImEW8sSya71rnSx9E3xHoOe3YdQ9bg2vBkBVWV9ykLmFJXy2toTP15Wy/6gPgL7t0xjdLYvR3bMYktvSxmeMMSdU74lFRNriltW/RVXrWJ+kYQlZYtm6xO3SWPAXiIlzkxlH3gOtepz+a9cjX0Uly7fuZe5aV222ZPNuyiuUhLgY8jq18Heb9euQTqyNzxhjPKEaY8nALcNyAW75lCeA54GfAg8Bq4BnVPWtUAQdLqeVWFRh3cdehde/XYVX3i1ehVe70AYaJofKfCzcsMsray5l9fZ9AKQlxrnxme4u0eRmJiO2irIxTVaoJkj+ErfP/Qzc/JLf4KZapAAXqeq/TjfQRqOyElb+yavwWgnN28HYx7wKr7Q6L2/IkhPiOLdna87t2RqAkgNHmbeu1N+ieX/VDgA6ZCQxypukOapbFlmpttSMMeZ4dbVYNgG3qupHItIFt5Pkc6r6o3AFWB9OqcWiCi+dBRXlrrur/1UQF/0D36rKptJD/rLmeetK2Xu4HIBebZu7QoDuWQzr3JLkhKAXcjDGNEKh6gorBzpV7RwpIoeAIaq6KmSRRsApd4Xt3+H2k2/CuzRWVCort+71J5r8jbspq6gkPlYYlNPCn2jO6JBOXGzT/XsyJhqFKrFUAG0DlkzZD5yhqhtCFmkEhG2CZBNwuKyC/E27/Ilm1bZ9qELzZnEM75rpL23u2irFxmeMaeRCNcYiwBsiUjWTPRF42Wu5+KnqZacWpmnskhJiOat7K87q3gqAXQfLmO/Nn5lbWMI/CtzuCm3TEl21WfdMRnXNonVaYiTDNsbUo7paLNOCeRFVvTlkEYWBtVjCZ3PpIeauc0UA8wpL2H3Ijc/0aJPqL2se1iWT1GY2PmNMQ2cz72thiSUyKiuVgu37/K2ZhRt2cdRXSVyMMLBjBqO9suYBHTOIt/EZYxocSyy1sMTSMBwpr2DJpt3+RLN8615UISUhluFdMr2usyy6t0618RljGoCQb/RlTKglxscyslsWI7tlAbDnUBmfr68anynl4y+LAWjVvJm/CGBUt0zapTeqhR6MaXKsxWIarKLdh5hXeKwQoPRgGQBdW6X4E83wrpmkJcZHOFJjmgbrCquFJZbGp7JS+err/f7dNBes38Xh8gpiY4QzstPdQprdsjgzpwUJcTY+Y0x9sMRSC0ssjd9RXwVfbN7jTzTLtuyhUiEpPpZhXVr6WzQ92zS3jc6MCRFLLLWwxBJ99h4uZ8H6Un+iWbfzIABZqQmM7JrlXxGgQ4aNzxhzqmzw3jQp6UnxXNC3LRf0bQvA9r2HmVt4LNHMWrYNgM5ZKYzq5lYEGNEli/RkG58xJtTC2mIRkXHAJNye96+o6hPVns/BraSc4Z0zUVVni0g88AowCJcMX1PVX3nXZHjP9cNto3yLqs6vLQ5rsTQtqsra4gPMWeuKAD5fX8rBsgpiBPp3SPeXNQ/KaWEbnRlTiwbXFSYiscAa3LL7RcAi4BpVLQg4Zwrwhaq+KCJ9gNmqmisi1wKXqerVIpIMFADnqupGEZkBfKaqr4hIApCsqntqi8USS9NWXlHJ0i17/Inmiy17qKhUEuNjGJJ7bHymT7s0G58xJkBD7AobChSq6noAEXkbGI9LElUUqNrcJB3YFnA8RUTigCSgDNgnImm4/WJuAlDVMu85Y04oPtYlkCG5LblvbA/2Hyln4YZjC2n+6r0vAWiRHM9Ir9psdLcsOrZMjnDkxjQO4UwsHYAtAY+LgGHVznkE+FBE7sZtJna+d/xPuCS0HUgG7lPVXSIyENgJTBORAcBi4F5VPVj9zUVkAjABICcnJ1T3ZKJA88R4zuvdhvN6twGgeN8Rt77Z2lLmFO7k78u3A5DTMtm/vtnIrpm0SIn+/XiMORXhTCw19SlU74e7Bpiuqk+LyAjgdRHph2vtVADtgRbAZyLyES7+QcDdqrpARCYBE4H/d9wbqU4BpoDrCgvRPZko1Dotke+emc13z8xGVVm386C/COBvy7bx1sLNiEDf9mn+RDMkt6WNzxjjCWdiKQI6BjzO5lhXV5VbcVsgo6rzRSQRyAKuBd5X1XKgWETmAnnAv4EiVV3gXf8nXGIxJiREhG6tU+nWOpUbR+biq6hkWdFef6KZOmcDL/1rPQlxMQzJbeFPNH3bpxNr4zOmiQpnYlkEdBeRzsBW4Gpcwgi0GTgPmC4ivXH7v+z0jn9LRN7AdYUNB55V1R0iskVEeqrqV961BRhTT+JiYxjcqQWDO7XgnvO6c/Coj4UbdzF3rUs0v37/K37NV6QnxTOya6Y/0XTKTLaFNE2TEe5y44uBZ3GlxFNV9XEReRTIV9VZXiXYy0AqrpvsQVX9UERSgWlAH1yX2jRVfdJ7zYG4cuMEYD1ws6ruri0Oqwoz9WXn/qPMW+eKAOasLWHb3iMAdMhI8k/SHNk1k6zUZhGO1JiT1+DKjRsSSywmHFSVjaWHXLXZ2hLmrSth3xEfAL3bpTG6m2vRDO3ckuQEm6tsGj5LLLWwxGIioaJSWbl1L3O81sziTbspq6gkPlYYlNPClTV3z6J/h3TibKMz0wBZYqmFJRbTEBwuq2DRxl3+QoBV2/YB0DwxjhFdMhnd3U3U7JKVYuMzpkFoiBMkjTEBkhJiObtHK87u0QqA0gNHme8tpPnZ2hI+LPgagHbpicfmz3TLpHXzxEiGbUydrMViTAO1uWp8prCEuetK2HOoHICebZp765tlMrRzJqnN7PdDEx7WFVYLSyymsamsVAq27/MnmoUbdnHUV0lcjHBmTgajumVxVvcszsjOIN7GZ0w9scRSC0ssprE7Ul7B4k27/Ylmxda9qEJqsziGd2np7zrr1jrVxmdMyNgYizFRLDE+llHeKswAew6VMX9dqT/RfLS6GIDWzZv5V2se1S2Ltuk2PmPqn7VYjIlCW3YdYt66EuZ4m53tOugW/e7WOtWfaIZ1aUlaom10ZoJnXWG1sMRimpLKSuXLHfv9Zc0LNpRypLyS2BhhQHa6P9GcmdOChDgbnzEnZomlFpZYTFN21FfBkk17/IlmedEeKhWSE2IZ2rmlf6JmzzbNbXzGfIMlllpYYjHmmL2Hy/ncmz8zp7CE9TvddkZZqc0Y1e3YQprtM5IiHKmJNBu8N8YEJT0pngv7tuXCvm0B2LbnsJs7U+jGaP6y1O1u0SUrxV8EMKJrJulJNj5jamYtFmPMCakqa74+4K82+3x9KYfKKogR6J+d4V9Ic3CnFjSLs43Oop11hdXCEosxp6bMV8myoj3M8fafWbplDxWVSmJ8DEM7Z/oTTe+2acTYRmdRxxJLLSyxGBMa+4+Us2D9Ln+LZm3xAQBapiQwsmumv+KsY8vkCEdqQsHGWIwx9a55Yjzn92nD+X3aAPD1viP+IoC5hSX8bfl2ADplJvuLAEZ0yaRFSkIkwzb1zFosxph6oaqs23nA6zYr5fP1pRw46kME+rVP9yeavNwWJMbb+ExjYF1htbDEYkz4lVdUsrxoL3PWutbMks278VUqCXExDMlt4U80fdunE2vjMw1Sg0wsIjIOmITb8/4VVX2i2vM5wAwgwztnoqrOFpF43L72g3Ddd6+p6q8CrosF8oGtqnpJXXFYYjEm8g4e9bFww7HxmS937AcgIzmekV2PzZ/JaZlsEzUbiAY3xuL98J8MjAWKgEUiMktVCwJOexiYqaovikgfYDaQC1wFNFPV/iKSDBSIyFuqutG77l5gNZAWnrsxxpyulGZxjOnVmjG9WgNQvP+IW0jTqzibvWIHANktkvxFACO7ZpKZ2iySYZsghHPwfihQqKrrAUTkbWA8EJhYlGPJIR3YFnA8RUTigCSgDNjnvU428G3gceD+er4HY0w9ad08kfEDOzB+YAdUlQ0lB/2FAH9fsZ23F20BoE+7NP+2zUNzW5KUYOMzDU04E0sHYEvA4yJgWLVzHgE+FJG7gRTgfO/4n3BJaDuQDNynqru8554FHgSa10/YxphwExG6tEqlS6tUrh+Ri6+ikpXb9rlEs7aE6XM3MuXf60mIjWFQpwxvfbNW9O9g4zMNQTgTS02fdvUBnmuA6ar6tIiMAF4XkX641k4F0B5oAXwmIh8BfYBiVV0sIufW+uYiE4AJADk5Oad1I8aY8IqLjWFgxwwGdszgzjHdOFxWwcKNu/yJ5qkP1/DUh2tonhj3jfkznbNSbHwmAsKZWIqAjgGPsznW1VXlVmAcgKrOF5FEIAu4FnhfVcuBYhGZC+QBZwKXicjFQCKQJiJvqOr3q7+5qk4BpoAbvA/pnRljwiopIZZzerTinB6tACg9cJR569xCmp+tLeGDVV8D0D490RUBdM9iZNcsWjW38ZlwCFtVmDc+sgY4D9gKLAKuVdVVAee8B/xBVaeLSG/gY1wX2oNAL+AWXFfYIuBqVV0ecO25wANWFWZM06aqbN51yF9tNrewlL2HywHo1ba5v9psaOeWpDSzOeIno8FVhamqT0TuAj7AlRJPVdVVIvIokK+qs4AfAy+LyH24brKbVFVFZDIwDViJ61KbFphUjDGmiojQKTOFTpkpXDesExWVSsG2ff5E8/rnm3h1zgbiYoRBOd78me6ZnJGdQXysbXQWCjZB0hjTpBwpryB/425/olm5bS+qkNosjuFdjm101rVVqo3PVNPgWizGGNMQJMbHMrq7Sx4Auw+WMX99qT/RfLS6GIA2ac383WajumXRJi0xkmE3KtZiMcaYAFt2HfLPn5m3rpRdB8sA6N461Z9ohnVpSfPEprfRWYNc0qWhsMRijAlGZaWyesc+/26aCzeUcqS8ktgYYWDHDH+iGdgxg4S46B+fscRSC0ssxphTcdRXwZJNe/wtmuVFe6hUSE6IZXiXY+ub9WgTneMzllhqYYnFGBMKew+VM399qVfWXML6koMAZKU28++mOapbFu0zkiIcaWhYYqmFJRZjTH3YuuewP8nMLSyh5IAbn+nSKsVfBDC8SybpSY1zfMYSSy0ssRhj6puq8tXX+/37zyzYsItDZRXECJyRneFPNIM6ZdAsrnEspGmJpRaWWIwx4Vbmq2Tplj3MWbuTOYUlLCvaS0Wlkhgfw9DOmf6us95t04hpoAtpWmKphSUWY0yk7TtSzoL1u/yFAIXFBwDITElgZLcsf6LJbpEc4UiPsQmSxhjTgKUlxjO2TxvG9mkDwI69R/xjM3MKS/jrMrdGb25msr/abETXTDKSEyIZdlCsxWKMMQ2MqlJYfMC/GsDn63dx4KgPEejfId2faAZ3akFifPjGZ6wrrBaWWIwxjUl5RSXLi/YwZ60rbV6yeTe+SqVZXAxDclv6E03f9vU7PmOJpRaWWIwxjdmBoz4WbdjFZ17F2Vdf7wcgIzmeUV2z/IkmJzO04zM2xmKMMVEqtVkcY3q1Zkyv1gAU7z/CvEK3kOactSX8fcV2ADq2TPKXNY/smkXLlPCMz1iLxRhjooiqsr7koH/b5vnrStl/1AdA3/ZpvH7rsFNOMNZiMcaYJkhE6Noqla6tUrlhRC6+ikpWbN3L3MISVmzdS4vk+p/1b4nFGGOiWFxsDGfmtODMnBZhe8/oX+fZGGNMWFliMcYYE1JhTSwiMk5EvhKRQhGZWMPzOSLyiYh8ISLLReRi73i8iMwQkRUislpE/ts73tE7f7WIrBKRe8N5P8YYY44XtjEWEYkFJgNjgSJgkYjMUtWCgNMeBmaq6osi0geYDeQCVwHNVLW/iCQDBSLyFnAU+LGqLhGR5sBiEflHtdc0xhgTRuFssQwFClV1vaqWAW8D46udo0Ca9306sC3geIqIxAFJQBmwT1W3q+oSAFXdD6wGOtTvbRhjjKlNOBNLB2BLwOMijk8CjwDfF5EiXGvlbu/4n4CDwHZgM/CUqu4KvFBEcoEzgQU1vbmITBCRfBHJ37lz52ndiDHGmBMLZ2KpaQGb6rMzrwGmq2o2cDHwuojE4Fo7FUB7oDPwYxHp4n9hkVTgHeBHqrqvpjdX1Smqmqeqea1atTr9uzHGGFOjcCaWIqBjwONsjnV1VbkVmAmgqvOBRCALuBZ4X1XLVbUYmAvkgRvYxyWVN1X13Xq9A2OMMXUK5wTJRUB3EekMbAWuxiWMQJuB84DpItIbl1h2ese/JSJvAMnAcOBZERHgVWC1qj4TbCCLFy8uEZFNp3APWUDJKVzXGNm9Rq+mdL9N6V6h/u+3UzAnhXWtMK98+FkgFpiqqo+LyKNAvqrO8irBXgZScd1kD6rqh15X1zSgD65LbZqqPikio4HPgBVApfc2/6Oqs+sp/vxg1smJBnav0asp3W9TuldoOPcb1iVdvB/4s6sd+2nA9wXAqBquO4ArOa5+fA41j90YY4yJEJt5b4wxJqQssZycKZEOIIzsXqNXU7rfpnSv0EDut0nux2KMMab+WIvFGGNMSFliMcYYE1KWWIJQ16rMjd2JVokWkZYi8g8RWet9Dd9OQfVMRGK9VbT/5j3uLCILvHv9g4iEZ3PweiYiGSLyJxH50vt8R0T553qf9294pYi8JSKJ0fLZishUESkWkZUBx2r8LMV5zvuZtVxEBoUzVkssdQhYlfki3Dyaa7z5NtHEh1slujdu8umd3j1OBD5W1e7Ax97jaHEvbtHSKv8L/Ma71924VSCiwSTcqhW9gAG4e47Kz1VEOgD3AHmq2g83X+5qoueznQ6Mq3bsRJ/lRUB3788E4MUwxQhYYglGMKsyN2q1rBI9HpjhnTYD+E5kIgwtEckGvg284j0W4Fu4xU4hSu5VRNKAs3GrU6CqZaq6hyj9XD1xQJK3EnoybuHaqPhsVfXfwK5qh0/0WY4HXlPncyBDRNqFJ1JLLMEIZlXmqFFtleg2qrodXPIBWkcuspB6FniQY6s1ZAJ7VNXnPY6Wz7gLbkmkaV633ysikkKUfq6quhV4CrcE1HZgL7CY6Pxsq5zos4zozy1LLHULZlXmqBDMKtGNnYhcAhSr6uLAwzWcGg2fcRwwCHhRVc/EbT0RFd1eNfHGF8bjVkBvD6TguoSqi4bPti4R/TdtiaVuwazK3OidYJXor6uaz97X4kjFF0KjgMtEZCOuW/NbuBZMhtd9AtHzGRcBRapatUfRn3CJJho/V4DzgQ2qulNVy4F3gZFE52db5USfZUR/blliqZt/VWavmuRqYFaEYwqpWlaJngXc6H1/I/CXcMcWaqr636qaraq5uM/yn6p6HfAJcKV3WrTc6w5gi4j09A6dBxQQhZ+rZzMwXESSvX/TVfcbdZ9tgBN9lrOAG7zqsOHA3qous3CwmfdBqGlV5giHFFInWiUaN84yE8jB/ae9qvrOnY2ZiJwLPKCql3gbx70NtAS+AL6vqkcjGV8oiMhAXJFCArAeuBn3C2VUfq4i8nPge7hKxy+A23BjC43+sxWRt4BzcUvjfw38DPgzNXyWXmJ9AVdFdgi4WVXzwxarJRZjjDGhZF1hxhhjQsoSizHGmJCyxGKMMSakLLEYY4wJKUssxhhjQsoSizGNnIjkioiKSF6kYzEGLLEYY4wJMUssxhhjQsoSizGnyVs240ERWScih0VkhYh833uuqpvqWhGZI/L/27ufEBnjOI7j76+Ig4scqXHQFg5IROEiW5scHBzcHFkX4UDt3qwRW9i0zHUPkpsTcWNrc5PDkoitdVBbRG071Hwdft9fPcbukueZ8afPq6aeeeb3/H7z1NR35pmn78fmInSrt22OvRFGNWdm783sSjGQKtY4HYFOTTObNrN621upRdjTrJlNmtn+Lpy+yA9UWETKO08KjzpBCoOrAw0zO1AYcwkYAbYAD4G7EUyVA6rukdqNbI25jsQ82QVgMPZtAg7zfVt0gKFYYzOpx93t6Fgt0lVq6SJSQuSbzAC97v64sP8q0AP0A2+AgdxjzsyWAC+AO+4+YGZDpP5WPe7eijFHgQawivQFcIYUZ3BznvewLtY45u6N2LeG1OF2j7uPV3/mIgtb+vMhIrKIjcAK4L6ZFb+lLQPeFp5P5A13b5nZkzgWYAMwkYtKGCc1jlwf8y8nRc8u5llhO7dI/y9CvOTfosIiUk6+nHyQ1F226CvzBy61MxYOYfJfnCOvlw5y99TgVpe7pfv0oRMpZxJoAjV3f9X2mCqM25k3oqX5DuB5YY5dcYks2w18AV4X1tjXwfMQqYx+sYiU4O6fzWwYGI6C8QhYSSokLeBBDD1uZi9JmTf9QA24Ea+NAieBUTO7Rsqqvwhcd/dZgNhfN7NmrLEa2ObueQ6Rv4YKi0h5g6TgpTOkYvEJeEq6Eyw7C5wiRQNPAYfcfRrA3d+ZWR9wOY77CNwiha1l54APsdbaWG+sc6ck8vt0V5hIBxXu2NrezQQ/kT9J/7GIiEilVFhERKRSuhQmIiKV0i8WERGplAqLiIhUSoVFREQqpcIiIiKVUmEREZFKfQMlFUK+AH69bwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# declare scoring histories\n",
    "scoring_history_t = nn_model.score_history()\n",
    "scoring_history_h = nn_model_h.score_history()\n",
    "# plot the loss for train and holdout frames \n",
    "plt.plot(scoring_history_t['epochs'], scoring_history_t['training_rmse']) \n",
    "plt.plot(scoring_history_h['epochs'], scoring_history_h['training_rmse']) \n",
    "labels = ['Training', 'Holdout']\n",
    "plt.title('RMSE Loss', fontsize=18)\n",
    "plt.ylabel('RMSE error', fontsize=14)\n",
    "plt.xlabel('epoch', fontsize=14)\n",
    "plt.legend(labels, loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The divergence in the loss of the holdout and training RMSE error shows that this model is not stable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drf prediction progress: |████████████████████████████████████████████████| 100%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>predict    </th><th style=\"text-align: right;\">    Austen</th><th style=\"text-align: right;\">      Blake</th><th style=\"text-align: right;\">    Bryant</th><th style=\"text-align: right;\">   Burgess</th><th style=\"text-align: right;\">   Carroll</th><th style=\"text-align: right;\">  Chesterton</th><th style=\"text-align: right;\">  Edgeworth</th><th style=\"text-align: right;\">  Melville</th><th style=\"text-align: right;\">  Milton</th><th style=\"text-align: right;\">  Shakespeare</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>Austen     </td><td style=\"text-align: right;\">1         </td><td style=\"text-align: right;\">0          </td><td style=\"text-align: right;\">0         </td><td style=\"text-align: right;\">0         </td><td style=\"text-align: right;\">0         </td><td style=\"text-align: right;\">    0       </td><td style=\"text-align: right;\">   0       </td><td style=\"text-align: right;\">  0       </td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">     0       </td></tr>\n",
       "<tr><td>Melville   </td><td style=\"text-align: right;\">0         </td><td style=\"text-align: right;\">0          </td><td style=\"text-align: right;\">0         </td><td style=\"text-align: right;\">0.00112994</td><td style=\"text-align: right;\">0         </td><td style=\"text-align: right;\">    0       </td><td style=\"text-align: right;\">   0       </td><td style=\"text-align: right;\">  0.99887 </td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">     0       </td></tr>\n",
       "<tr><td>Shakespeare</td><td style=\"text-align: right;\">0         </td><td style=\"text-align: right;\">0          </td><td style=\"text-align: right;\">0.0177187 </td><td style=\"text-align: right;\">0         </td><td style=\"text-align: right;\">0         </td><td style=\"text-align: right;\">    0       </td><td style=\"text-align: right;\">   0       </td><td style=\"text-align: right;\">  0       </td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">     0.982281</td></tr>\n",
       "<tr><td>Bryant     </td><td style=\"text-align: right;\">0         </td><td style=\"text-align: right;\">0          </td><td style=\"text-align: right;\">0.998693  </td><td style=\"text-align: right;\">0         </td><td style=\"text-align: right;\">0.00130726</td><td style=\"text-align: right;\">    0       </td><td style=\"text-align: right;\">   0       </td><td style=\"text-align: right;\">  0       </td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">     0       </td></tr>\n",
       "<tr><td>Shakespeare</td><td style=\"text-align: right;\">0.0380183 </td><td style=\"text-align: right;\">0.000258628</td><td style=\"text-align: right;\">0         </td><td style=\"text-align: right;\">0         </td><td style=\"text-align: right;\">0         </td><td style=\"text-align: right;\">    0       </td><td style=\"text-align: right;\">   0       </td><td style=\"text-align: right;\">  0       </td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">     0.961723</td></tr>\n",
       "<tr><td>Melville   </td><td style=\"text-align: right;\">0         </td><td style=\"text-align: right;\">0          </td><td style=\"text-align: right;\">0         </td><td style=\"text-align: right;\">0.0369532 </td><td style=\"text-align: right;\">0.00226244</td><td style=\"text-align: right;\">    0       </td><td style=\"text-align: right;\">   0       </td><td style=\"text-align: right;\">  0.960784</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">     0       </td></tr>\n",
       "<tr><td>Chesterton </td><td style=\"text-align: right;\">0.083952  </td><td style=\"text-align: right;\">0.111936   </td><td style=\"text-align: right;\">0.125912  </td><td style=\"text-align: right;\">0         </td><td style=\"text-align: right;\">0         </td><td style=\"text-align: right;\">    0.622232</td><td style=\"text-align: right;\">   0.055968</td><td style=\"text-align: right;\">  0       </td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">     0       </td></tr>\n",
       "<tr><td>Burgess    </td><td style=\"text-align: right;\">0         </td><td style=\"text-align: right;\">0          </td><td style=\"text-align: right;\">0         </td><td style=\"text-align: right;\">1         </td><td style=\"text-align: right;\">0         </td><td style=\"text-align: right;\">    0       </td><td style=\"text-align: right;\">   0       </td><td style=\"text-align: right;\">  0       </td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">     0       </td></tr>\n",
       "<tr><td>Melville   </td><td style=\"text-align: right;\">0         </td><td style=\"text-align: right;\">0          </td><td style=\"text-align: right;\">0.012633  </td><td style=\"text-align: right;\">0         </td><td style=\"text-align: right;\">0.00199468</td><td style=\"text-align: right;\">    0       </td><td style=\"text-align: right;\">   0       </td><td style=\"text-align: right;\">  0.985372</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">     0       </td></tr>\n",
       "<tr><td>Chesterton </td><td style=\"text-align: right;\">0.00387918</td><td style=\"text-align: right;\">0          </td><td style=\"text-align: right;\">0.00258672</td><td style=\"text-align: right;\">0         </td><td style=\"text-align: right;\">0         </td><td style=\"text-align: right;\">    0.993534</td><td style=\"text-align: right;\">   0       </td><td style=\"text-align: right;\">  0       </td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">     0       </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training\n",
    "rf_model.predict(valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drf prediction progress: |████████████████████████████████████████████████| 100%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>predict    </th><th style=\"text-align: right;\">   Austen</th><th style=\"text-align: right;\">     Blake</th><th style=\"text-align: right;\">    Bryant</th><th style=\"text-align: right;\">  Burgess</th><th style=\"text-align: right;\">   Carroll</th><th style=\"text-align: right;\">  Chesterton</th><th style=\"text-align: right;\">  Edgeworth</th><th style=\"text-align: right;\">  Melville</th><th style=\"text-align: right;\">    Milton</th><th style=\"text-align: right;\">  Shakespeare</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>Edgeworth  </td><td style=\"text-align: right;\">0        </td><td style=\"text-align: right;\">0         </td><td style=\"text-align: right;\">0         </td><td style=\"text-align: right;\">0        </td><td style=\"text-align: right;\">0         </td><td style=\"text-align: right;\">   0        </td><td style=\"text-align: right;\">  1        </td><td style=\"text-align: right;\"> 0        </td><td style=\"text-align: right;\">0         </td><td style=\"text-align: right;\">   0         </td></tr>\n",
       "<tr><td>Melville   </td><td style=\"text-align: right;\">0.0381679</td><td style=\"text-align: right;\">0         </td><td style=\"text-align: right;\">0         </td><td style=\"text-align: right;\">0.0458015</td><td style=\"text-align: right;\">0         </td><td style=\"text-align: right;\">   0        </td><td style=\"text-align: right;\">  0        </td><td style=\"text-align: right;\"> 0.916031 </td><td style=\"text-align: right;\">0         </td><td style=\"text-align: right;\">   0         </td></tr>\n",
       "<tr><td>Bryant     </td><td style=\"text-align: right;\">0        </td><td style=\"text-align: right;\">0         </td><td style=\"text-align: right;\">0.861111  </td><td style=\"text-align: right;\">0        </td><td style=\"text-align: right;\">0.111111  </td><td style=\"text-align: right;\">   0.0277778</td><td style=\"text-align: right;\">  0        </td><td style=\"text-align: right;\"> 0        </td><td style=\"text-align: right;\">0         </td><td style=\"text-align: right;\">   0         </td></tr>\n",
       "<tr><td>Burgess    </td><td style=\"text-align: right;\">0        </td><td style=\"text-align: right;\">0         </td><td style=\"text-align: right;\">0.0229695 </td><td style=\"text-align: right;\">0.651308 </td><td style=\"text-align: right;\">0.138128  </td><td style=\"text-align: right;\">   0        </td><td style=\"text-align: right;\">  0.100455 </td><td style=\"text-align: right;\"> 0.0840652</td><td style=\"text-align: right;\">0.00154653</td><td style=\"text-align: right;\">   0.00152724</td></tr>\n",
       "<tr><td>Bryant     </td><td style=\"text-align: right;\">0.030303 </td><td style=\"text-align: right;\">0         </td><td style=\"text-align: right;\">0.939394  </td><td style=\"text-align: right;\">0        </td><td style=\"text-align: right;\">0.030303  </td><td style=\"text-align: right;\">   0        </td><td style=\"text-align: right;\">  0        </td><td style=\"text-align: right;\"> 0        </td><td style=\"text-align: right;\">0         </td><td style=\"text-align: right;\">   0         </td></tr>\n",
       "<tr><td>Shakespeare</td><td style=\"text-align: right;\">0.0145743</td><td style=\"text-align: right;\">0.00754817</td><td style=\"text-align: right;\">0.00324075</td><td style=\"text-align: right;\">0.0441421</td><td style=\"text-align: right;\">0.0161117 </td><td style=\"text-align: right;\">   0.0112404</td><td style=\"text-align: right;\">  0.0235792</td><td style=\"text-align: right;\"> 0.128082 </td><td style=\"text-align: right;\">0.0737111 </td><td style=\"text-align: right;\">   0.677771  </td></tr>\n",
       "<tr><td>Shakespeare</td><td style=\"text-align: right;\">0.0607303</td><td style=\"text-align: right;\">0.246589  </td><td style=\"text-align: right;\">0.109245  </td><td style=\"text-align: right;\">0.240757 </td><td style=\"text-align: right;\">0.00566488</td><td style=\"text-align: right;\">   0        </td><td style=\"text-align: right;\">  0.0332006</td><td style=\"text-align: right;\"> 0        </td><td style=\"text-align: right;\">0.00751463</td><td style=\"text-align: right;\">   0.296298  </td></tr>\n",
       "<tr><td>Burgess    </td><td style=\"text-align: right;\">0        </td><td style=\"text-align: right;\">0         </td><td style=\"text-align: right;\">0         </td><td style=\"text-align: right;\">1        </td><td style=\"text-align: right;\">0         </td><td style=\"text-align: right;\">   0        </td><td style=\"text-align: right;\">  0        </td><td style=\"text-align: right;\"> 0        </td><td style=\"text-align: right;\">0         </td><td style=\"text-align: right;\">   0         </td></tr>\n",
       "<tr><td>Edgeworth  </td><td style=\"text-align: right;\">0        </td><td style=\"text-align: right;\">0         </td><td style=\"text-align: right;\">0.0967742 </td><td style=\"text-align: right;\">0        </td><td style=\"text-align: right;\">0         </td><td style=\"text-align: right;\">   0        </td><td style=\"text-align: right;\">  0.903226 </td><td style=\"text-align: right;\"> 0        </td><td style=\"text-align: right;\">0         </td><td style=\"text-align: right;\">   0         </td></tr>\n",
       "<tr><td>Edgeworth  </td><td style=\"text-align: right;\">0        </td><td style=\"text-align: right;\">0         </td><td style=\"text-align: right;\">0         </td><td style=\"text-align: right;\">0        </td><td style=\"text-align: right;\">0         </td><td style=\"text-align: right;\">   0        </td><td style=\"text-align: right;\">  1        </td><td style=\"text-align: right;\"> 0        </td><td style=\"text-align: right;\">0         </td><td style=\"text-align: right;\">   0         </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# holdout\n",
    "rf_model_h.predict(valid_h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tables above show the accuracy and error of the random forest model when classifiying authors in the validation frame. While the tables show some error in the model, they also could be used to show which authors use similar words. \n",
    "\n",
    "Overall, the model shows relatively good accuracy when classifiying authors. Furthermore, the random forest shows much less classification error than the neural network model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# declare scoring histories\n",
    "scoring_history_t = rf_model.score_history()\n",
    "scoring_history_h = rf_model_h.score_history()\n",
    "# plot the loss for train and holdout frames \n",
    "plt.plot(scoring_history_t['number_of_trees'], scoring_history_t['training_rmse']) \n",
    "plt.plot(scoring_history_h['number_of_trees'], scoring_history_h['training_rmse']) \n",
    "labels = ['Training', 'Holdout']\n",
    "plt.title('RMSE Loss', fontsize=18)\n",
    "plt.ylabel('RMSE error', fontsize=14)\n",
    "plt.xlabel('Number of trees', fontsize=14)\n",
    "plt.legend(labels, loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The random forest model is stable when comparing the RMSE error of the holdout and training frames. 12-14 trees seems to be the optimal number for the model, as shown in the classification error scores for the random forest model and in the plot above. \n",
    "\n",
    "Overall, the neural network seems to perform better when classifiying authors, and the tfidf vectors for these words, by word usage when looking at the r2 value, but the random forest has much less classification error. The neural network is also not stable and should not be used to classify authors with this data. The random forest is the better model for this dataset due to the low classification error and stability. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
